{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqMVhqvs7oj8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "# Market Complexity Dashboard: A Framework for Advanced Financial Time Series Analysis\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![Seaborn](https://img.shields.io/badge/Seaborn-%233776AB.svg?style=flat&logo=seaborn&logoColor=white)](https://seaborn.pydata.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2507.23414-b31b1b.svg)](https://arxiv.org/abs/2507.23414)\n",
        "[![Research](https://img.shields.io/badge/Research-Financial%20Complexity-green)](https://github.com/chirindaopensource/multifractal_multiscale_analysis)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Econophysics-blue)](https://github.com/chirindaopensource/multifractal_multiscale_analysis)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Nonlinear%20Time%20Series-orange)](https://github.com/chirindaopensource/multifractal_multiscale_analysis)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/multifractal_multiscale_analysis)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/multifractal_multiscale_analysis`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Complexity of Financial Time Series: Multifractal and Multiscale Entropy Analyses\"** by:\n",
        "\n",
        "*   Oday Masoudi\n",
        "*   Farhad Shahbazi\n",
        "*   Mohammad Sharifi\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for quantifying the dynamic complexity of financial assets. It moves beyond traditional, linear measures of risk (like standard deviation) to deliver a deeper, more nuanced characterization of market behavior based on concepts from information theory and fractal geometry. The goal is to provide a transparent, robust, and computationally efficient toolkit for researchers and practitioners to analyze nonlinearity, predictability, and the rich scaling structure of financial time series.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: market_complexity_dashboard_generator](#key-callable-market_complexity_dashboard_generator)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Complexity of Financial Time Series: Multifractal and Multiscale Entropy Analyses.\" The core of this repository is the iPython Notebook `multifractal_multiscale_analysis_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation and cleansing to the final generation of complexity metrics, statistical tests, and publication-quality reports.\n",
        "\n",
        "Traditional financial analysis often relies on the first two moments of the return distribution (mean and variance). However, these measures fail to capture the rich, nonlinear, and non-stationary dynamics that characterize modern financial markets. This project provides the tools to explore this hidden structure.\n",
        "\n",
        "This codebase enables users to:\n",
        "-   Rigorously validate, cleanse, and prepare financial time series data using a sophisticated, multi-stage protocol.\n",
        "-   Quantify the irregularity and predictability of assets across dozens of time scales using Refined Composite Multiscale Sample Entropy (RCMSE).\n",
        "-   Characterize the \"fractal fingerprint\" of an asset's volatility using Multifractal Detrended Fluctuation Analysis (MF-DFA).\n",
        "-   Statistically test for the presence of significant nonlinear correlations by comparing results against a computationally generated null model (surrogate data).\n",
        "-   Automatically generate a complete \"complexity dashboard,\" including summary tables, scientific visualizations, and validation reports.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in econophysics, a field that applies concepts from statistical physics and nonlinear dynamics to understand economic and financial systems.\n",
        "\n",
        "**1. Refined Composite Multiscale Sample Entropy (RCMSE):**\n",
        "Sample Entropy is a measure of the unpredictability or irregularity of a time series. It quantifies the conditional probability that two similar patterns of length `m` will remain similar when a new data point is added. A lower entropy implies more regularity and predictability. RCMSE extends this concept by:\n",
        "-   **Multiscale Analysis:** Applying a \"coarse-graining\" procedure to analyze the series at different time scales (horizons), revealing how predictability changes from short-term to long-term.\n",
        "-   **Refined Composite Method:** Using overlapping windows during coarse-graining to produce more stable and reliable entropy estimates, especially for the finite time series common in finance.\n",
        "The final calculation is based on the logarithm of the ratio of aggregated pattern counts:\n",
        "$$\n",
        "\\text{RCMSE}(x, \\tau, m, r) = -\\ln\\left(\\frac{\\sum_{k=1}^{\\tau} n^{m+1}_{(k,\\tau)}}{\\sum_{k=1}^{\\tau} n^{m}_{(k,\\tau)}}\\right)\n",
        "$$\n",
        "\n",
        "**2. Multifractal Detrended Fluctuation Analysis (MF-DFA):**\n",
        "Financial time series often exhibit multifractality, meaning their scaling properties (how volatility changes with the measurement scale) are not uniform but vary over time. MF-DFA is a powerful technique to quantify this phenomenon.\n",
        "-   **Detrended Fluctuation:** The method systematically removes local trends from the data at different scales `s`, allowing for an accurate measurement of the underlying fluctuation dynamics.\n",
        "-   **q-order Moments:** It calculates a `q`-dependent fluctuation function `F_q(s)` that magnifies either large (`q > 0`) or small (`q < 0`) fluctuations.\n",
        "-   **Singularity Spectrum:** The final output is the singularity spectrum, `f(α)`. This spectrum is a \"fingerprint\" of the asset's complexity. The width of the spectrum, `Δα`, is a direct measure of the degree of multifractality: a wider spectrum implies a more complex, heterogeneous process with a richer mixture of behaviors.\n",
        "$$\n",
        "F_q(s) \\sim s^{h(q)} \\quad \\xrightarrow{\\text{Legendre Transform}} \\quad (\\alpha, f(\\alpha))\n",
        "$$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`multifractal_multiscale_analysis_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Data Pipeline:** A robust validation and cleansing module that performs structural checks, advanced gap-filling, and outlier removal.\n",
        "-   **High-Performance Analytics:** Elite-grade, vectorized implementations of RCMSE and MF-DFA using advanced NumPy features for maximum speed and memory efficiency.\n",
        "-   **Statistical Rigor:** A parallelized surrogate data analysis framework to test the statistical significance of the complexity measures.\n",
        "-   **Automated Orchestration:** A master function that runs the entire end-to-end workflow, from raw data to final reports, with a single call.\n",
        "-   **Comprehensive Reporting:** Automated generation of publication-quality summary tables and scientific visualizations that replicate the key exhibits from the source paper.\n",
        "-   **Built-in Quality Control:** An automated verification module to check the plausibility of results against theoretical expectations and known benchmarks.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Validation and Preparation (Task 1):** The pipeline ingests raw price data, performs over a dozen structural and quality checks, and applies a sophisticated cleansing protocol including targeted gap-filling and outlier removal.\n",
        "2.  **Preprocessing (Task 2):** It transforms clean prices into log-returns, calculates their key statistical moments, and generates a shuffled surrogate dataset for control analysis.\n",
        "3.  **RCMSE Analysis (Task 3):** It computes the full RCMSE profile and total complexity score for each asset.\n",
        "4.  **MF-DFA Analysis (Task 4):** It computes the generalized Hurst exponents and the full singularity spectrum (`α`, `f(α)`, `Δα`) for each asset.\n",
        "5.  **Control Analysis (Task 5):** It runs the RCMSE and MF-DFA analyses on an ensemble of shuffled datasets in parallel to generate a null distribution for the complexity metrics.\n",
        "6.  **Statistical Testing (Task 7):** It performs formal hypothesis tests (empirical p-values, z-scores) to determine if the complexity of the original data is statistically significant.\n",
        "7.  **Reporting and Verification (Task 8):** It synthesizes all outputs into summary tables, plots, and a final verification report.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `multifractal_multiscale_analysis_draft.ipynb` notebook is structured as a logical pipeline with modular functions for each task:\n",
        "\n",
        "-   **Task 1:** Data Validation and Cleansing (`validate_and_prepare_data`).\n",
        "-   **Task 2:** Preprocessing and Characterization (`preprocess_and_characterize_data`).\n",
        "-   **Task 3:** RCMSE Implementation (`compute_rcmse_profile`).\n",
        "-   **Task 4:** MF-DFA Implementation (`compute_mfdfa_analysis`).\n",
        "-   **Task 5:** Shuffling Control Analysis (`conduct_shuffling_analysis`).\n",
        "-   **Task 6:** Primary Pipeline Orchestrator (`run_complexity_analysis_pipeline`).\n",
        "-   **Task 7:** Robustness and Statistical Significance Testing.\n",
        "-   **Task 8:** Output Generation, Verification, and Master Orchestrator.\n",
        "\n",
        "## Key Callable: market_complexity_dashboard_generator\n",
        "\n",
        "The central function in this project is `market_complexity_dashboard_generator`. It orchestrates the entire analytical workflow from raw data to a final, comprehensive report object.\n",
        "\n",
        "```python\n",
        "def market_complexity_dashboard_generator(\n",
        "    df_prices: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    run_sensitivity: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Master orchestrator for the entire Market Complexity Dashboard framework.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `matplotlib`, `seaborn`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/multifractal_multiscale_analysis.git\n",
        "    cd multifractal_multiscale_analysis\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy matplotlib seaborn\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two inputs passed to the `market_complexity_dashboard_generator` function:\n",
        "\n",
        "1.  **`df_prices`**: A `pandas.DataFrame` where the index is a `DatetimeIndex` and columns are individual asset prices.\n",
        "2.  **`study_config`**: A nested Python dictionary that controls all hyperparameters of the analysis. A fully specified example is provided in the notebook.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `multifractal_multiscale_analysis_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your asset prices into a DataFrame and define the `study_config` dictionary.\n",
        "2.  **Execute Pipeline:** Call the master orchestrator function:\n",
        "    ```python\n",
        "    dashboard = market_complexity_dashboard_generator(\n",
        "        df_prices=my_price_data_df,\n",
        "        study_config=my_config\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned `dashboard` dictionary. For example, to view the main MF-DFA summary table:\n",
        "    ```python\n",
        "    mdfa_summary_table = dashboard['summary_tables']['Table3_MFDFA_Spectrum_Widths']\n",
        "    print(mdfa_summary_table)\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `market_complexity_dashboard_generator` function returns a single, comprehensive dictionary with the following top-level keys:\n",
        "\n",
        "-   `main_analysis_results`: A deeply nested dictionary containing all raw numerical results from the RCMSE, MF-DFA, and shuffling analyses.\n",
        "-   `pipeline_validation_report`: A dictionary summarizing the results of the automated quality control checks.\n",
        "-   `sensitivity_analysis_results`: A `pd.DataFrame` with the results of the parameter sensitivity analysis (if run).\n",
        "-   `summary_tables`: A dictionary of `pd.DataFrame` objects, each representing a publication-quality summary table.\n",
        "-   `figure_paths`: A dictionary mapping figure names to the file paths where they were saved.\n",
        "-   `benchmark_verification_report`: A report detailing the comparison of key results against known benchmarks.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "multifractal_multiscale_analysis/\n",
        "│\n",
        "├── multifractal_multiscale_analysis_draft.ipynb  # Main implementation notebook   \n",
        "├── requirements.txt                              # Python package dependencies\n",
        "├── LICENSE                                       # MIT license file\n",
        "└── README.md                                     # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the master `study_config` dictionary. Users can easily modify:\n",
        "-   The `embedding_dimension_m` and `tolerance_r_factor` for RCMSE.\n",
        "-   The `q_range` and `polynomial_order` for MF-DFA.\n",
        "-   The `time_series_length` and `study_end_date` to change the analysis window.\n",
        "-   The `parameter_grid` in the `run_parameter_sensitivity_analysis` function to test different hyperparameters.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{masoudi2025complexity,\n",
        "  title={Complexity of Financial Time Series: Multifractal and Multiscale Entropy Analyses},\n",
        "  author={Masoudi, Oday and Shahbazi, Farhad and Sharifi, Mohammad},\n",
        "  journal={arXiv preprint arXiv:2507.23414},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of \"Complexity of Financial Time Series: Multifractal and Multiscale Entropy Analyses\".\n",
        "GitHub repository: https://github.com/chirindaopensource/multifractal_multiscale_analysis\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Oday Masoudi, Farhad Shahbazi, and Mohammad Sharifi for their clear and insightful research.\n",
        "-   Thanks to the developers of the scientific Python ecosystem (`numpy`, `pandas`, `scipy`, `matplotlib`) that makes this work possible.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `multifractal_multiscale_analysis_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "crL66W9DLJlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Complexity of Financial Time Series: Multifractal and Multiscale Entropy Analyses*\"\n",
        "\n",
        "Authors: Oday Masoudi, Farhad Shahbazi, Mohammad Sharifi\n",
        "\n",
        "E-Journal Submission Date: 31 July 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2507.23414\n",
        "\n",
        "Abstract:\n",
        "\n",
        "We employed Multifractal Detrended Fluctuation Analysis (MF-DFA) and Refined Composite Multiscale Sample Entropy (RCMSE) to investigate the complexity of Bitcoin, GBP/USD, gold, and natural gas price log-return time series. This study provides a comparative analysis of these markets and offers insights into their predictability and associated risks. Each tool presents a unique method to quantify time series complexity. The RCMSE and MF-DFA methods demonstrate a higher complexity for the Bitcoin time series than others. It is discussed that the increased complexity of Bitcoin may be attributable to the presence of higher nonlinear correlations within its log-return time series.\n",
        "\n"
      ],
      "metadata": {
        "id": "i5dZsDjrC0X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Overall Assessment**\n",
        "\n",
        "The paper presents a comparative analysis of the complexity of four distinct financial assets—Bitcoin (cryptocurrency), GBP/USD (forex), gold (precious metal), and natural gas (commodity)—using two sophisticated time-series analysis techniques. The authors' primary contribution is not the development of new methods but rather the application and synthesis of these tools to provide a quantitative, comparative measure of complexity and to infer the nature of the underlying market dynamics. The choice of Refined Composite Multiscale Sample Entropy (RCMSE) is a methodologically sound decision that strengthens their analysis.\n",
        "\n",
        "--\n",
        "\n",
        "### **Step-by-Step Summary**\n",
        "\n",
        "#### **Step 1: Problem Formulation and Data Preparation**\n",
        "\n",
        "*   **Objective:** The central goal is to quantify and compare the \"complexity\" of different financial time series. Complexity here is not a vague term; it is specifically investigated through two lenses: (1) the regularity and predictability of the series across different time scales, and (2) the fractal nature and long-range correlations within the series.\n",
        "*   **Assets Chosen:** The authors selected a diverse set of assets to ensure a broad comparison:\n",
        "    *   **Bitcoin (BTC):** A highly volatile, relatively new digital asset class.\n",
        "    *   **GBP/USD:** A major, mature foreign exchange pair.\n",
        "    *   **Gold:** A traditional safe-haven asset and precious metal.\n",
        "    *   **Natural Gas:** A volatile energy commodity.\n",
        "*   **Data Transformation:** Crucially, the authors do not analyze the raw price series. They correctly convert the daily closing prices into **logarithmic returns**: `ln(P_t / P_{t-1})`. This is standard and essential practice in financial econometrics for two reasons:\n",
        "    1.  **Stationarity:** Return series are typically more stationary (or at least have weaker trends) than price series, which is a prerequisite for many statistical models.\n",
        "    2.  **Interpretation:** Log returns approximate the percentage change and are indicative of market volatility and risk.\n",
        "\n",
        "#### **Step 2: Methodology I - Quantifying Regularity with Refined Composite Multiscale Sample Entropy (RCMSE)**\n",
        "\n",
        "This method measures how predictable a time series is at different time horizons.\n",
        "\n",
        "*   **Core Idea (Sample Entropy - SampEn):** At its heart, SampEn quantifies regularity. It calculates the probability that two similar short sequences of data points will remain similar when the next data point is added. A low SampEn value implies high regularity and predictability (i.e., patterns repeat), while a high value implies randomness and unpredictability.\n",
        "*   **The \"Multiscale\" Component (MSE):** Financial dynamics can differ at short (e.g., daily) and long (e.g., monthly) time scales. The multiscale approach addresses this by creating \"coarse-grained\" versions of the original time series. For a scale factor `τ`, it averages non-overlapping blocks of `τ` data points. SampEn is then calculated for each of these new, shorter, coarse-grained series.\n",
        "*   **The \"Refined Composite\" Improvement (RCMSE):** A known issue with standard MSE is that as the scale `τ` increases, the coarse-grained series becomes very short, leading to unreliable entropy estimates. RCMSE, the method used in this paper, improves this by creating `τ` different overlapping coarse-grained series for each scale factor `τ`. It then averages the pattern-matching statistics across all these series before calculating a single, more robust entropy value for that scale. This is the paper's key methodological strength in the entropy analysis.\n",
        "\n",
        "#### **Step 3: Methodology II - Characterizing Fractal Nature with Multifractal Detrended Fluctuation Analysis (MF-DFA)**\n",
        "\n",
        "This method investigates whether the time series exhibits self-similarity and long-range correlations, and whether these properties are uniform (monofractal) or varied (multifractal).\n",
        "\n",
        "*   **Core Idea (DFA):** DFA is designed to detect long-range correlations in non-stationary signals. It measures how the average fluctuation (or \"detrended\" volatility) of a signal grows with the size of the time window over which it is observed. The relationship is typically a power law, `F(s) ~ s^H`, where `H` is the Hurst exponent.\n",
        "    *   `H = 0.5`: No long-range correlation (a random walk).\n",
        "    *   `H > 0.5`: Persistent, long-range correlations.\n",
        "    *   `H < 0.5`: Anti-persistent correlations.\n",
        "*   **The \"Multifractal\" Component (MF-DFA):** Financial series are often more complex than a single Hurst exponent can describe. MF-DFA generalizes this by calculating a whole spectrum of scaling exponents, `h(q)`, corresponding to different statistical moments `q`. This leads to the **singularity spectrum, f(α)**.\n",
        "*   **Key Output:** The primary result from MF-DFA is the **width of the singularity spectrum (Δα = α_max - α_min)**. A wider spectrum (large Δα) indicates a richer multifractal structure, meaning the local scaling behavior varies significantly throughout the series. This is interpreted as a higher degree of complexity.\n",
        "\n",
        "#### **Step 4: Execution and Key Findings**\n",
        "\n",
        "The authors apply these two methods to the log-return series of the four assets.\n",
        "\n",
        "1.  **RCMSE Analysis Results:**\n",
        "    *   **Scale-Dependent Behavior (Fig. 6):** At short time scales (τ < 10), Bitcoin has the *lowest* entropy, suggesting it is the most regular and predictable on a day-to-day basis. In contrast, GBP/USD is the most random.\n",
        "    *   **Crossover Effect:** As the time scale increases, the roles reverse. At larger scales, Bitcoin's entropy becomes the *highest*, indicating it is the most unpredictable and irregular over longer horizons.\n",
        "    *   **Overall Complexity (Table 2):** By summing the entropy values across all scales (a common, though not unique, definition of total complexity), Bitcoin is found to be the most complex asset overall.\n",
        "\n",
        "2.  **MF-DFA Analysis Results:**\n",
        "    *   **Singularity Spectrum Width (Fig. 8, Table 3):** The MF-DFA results corroborate the RCMSE findings. Bitcoin exhibits the widest singularity spectrum (Δα = 0.62), while Natural Gas has the narrowest (Δα = 0.21). This confirms that Bitcoin's dynamics are the most multifractal and, by this measure, the most complex.\n",
        "\n",
        "3.  **The Critical Test: Shuffling Analysis:**\n",
        "    *   **Purpose:** To determine the source of the observed complexity. Is it due to (a) the distribution of returns (e.g., \"fat tails\") or (b) the temporal ordering and correlations in the data? Shuffling the time series preserves the distribution but destroys all temporal correlations.\n",
        "    *   **Results:**\n",
        "        *   For entropy (Fig. 7), shuffling significantly *increases* the sample entropy of Bitcoin, while having little effect on the others. This implies the original Bitcoin series had significant temporal structure/regularity that was destroyed by shuffling.\n",
        "        *   For multifractality (Fig. 10), shuffling dramatically *narrows* the singularity spectrum for all assets, especially Bitcoin.\n",
        "    *   **Interpretation:** This is a crucial finding. It demonstrates that the rich multifractality observed is primarily due to **long-range temporal correlations** and not just the fat-tailed nature of the returns distribution.\n",
        "\n",
        "#### **Step 5: Synthesis and Final Conclusion**\n",
        "\n",
        "The paper synthesizes these findings to draw a cohesive picture.\n",
        "\n",
        "*   **Bitcoin's Unique Complexity:** Bitcoin is demonstrably more complex than the traditional assets analyzed. This complexity is multifaceted: it has the highest integrated multiscale entropy and the richest multifractal structure.\n",
        "*   **Source of Complexity:** The authors combine the shuffling analysis with the standard Hurst exponent calculation (which shows H ≈ 0.5 for all assets, indicating no *linear* correlation). The conclusion is powerful: the complexity and multifractality in these financial assets, particularly Bitcoin, arise from **significant non-linear temporal correlations**.\n",
        "*   **Implications:** The study shows that financial assets can have mixed characteristics. Bitcoin, for instance, exhibits more predictable behavior in the short term but becomes highly unstable and unpredictable over the long term. This complex dynamic is a hallmark of systems driven by a combination of factors like investor sentiment, algorithmic trading, and external shocks.\n",
        "\n",
        "### **Critique and Further Thoughts**\n",
        "\n",
        "This is a solid piece of work that correctly applies established econophysics methods.\n",
        "\n",
        "*   **Strengths:** The comparative nature of the study is its main strength. The choice of RCMSE over standard MSE is technically robust. The use of shuffling to disentangle the sources of multifractality is excellent and leads to their most important conclusion about non-linear correlations.\n",
        "*   **Limitations and Avenues for Future Research:**\n",
        "    *   The analysis is purely statistical. It masterfully answers \"what\" the dynamics look like but cannot answer \"why.\" The next step would be to link these complexity measures to underlying economic drivers, market microstructure data (like order book dynamics), or investor behavior.\n",
        "    *   The definition of \"complexity\" as the sum of entropies is a heuristic. While reasonable, it's one of many possible definitions.\n",
        "    *   It would be interesting to compare these physics-based complexity measures with standard econometric measures of risk and volatility, such as those from GARCH models, to see what new information they provide. Do periods of high multifractality correspond to periods of high volatility as measured by GARCH?\n",
        "\n",
        "In summary, the authors provide compelling quantitative evidence that Bitcoin's market dynamics are fundamentally more complex and multifractal than those of established assets, and they correctly identify non-linear temporal dependencies as the primary source of this complexity."
      ],
      "metadata": {
        "id": "CyLwFUOWDtXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules\n"
      ],
      "metadata": {
        "id": "C-6qBQ9MlZ9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Market Complexity Dashboard: A Framework for Advanced Financial Time Series Analysis\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Complexity of Financial Time Series:\n",
        "#  Multifractal and Multiscale Entropy Analyses\" by Masoudi, Shahbazi, and\n",
        "#  Sharifi (2025). It delivers a robust system for quantifying the dynamic\n",
        "#  complexity of financial assets, moving beyond traditional linear measures to\n",
        "#  characterize nonlinearity, predictability across time scales, and fractal\n",
        "#  scaling behavior.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Refined Composite Multiscale Sample Entropy (RCMSE) for measuring\n",
        "#    irregularity and predictability across multiple time horizons.\n",
        "#  • Multifractal Detrended Fluctuation Analysis (MF-DFA) for characterizing\n",
        "#    the heterogeneous scaling properties and identifying the richness of\n",
        "#    fractal dynamics.\n",
        "#  • Surrogate Data Analysis (Shuffling) to rigorously test for the presence\n",
        "#    of significant nonlinear temporal correlations against a null hypothesis\n",
        "#    of randomness.\n",
        "#  • Comprehensive data validation, cleansing, and preparation pipeline to\n",
        "#    ensure the integrity and quality of the input financial data.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • High-performance, vectorized algorithms for RCMSE and MF-DFA using\n",
        "#    advanced NumPy features (broadcasting, stride tricks).\n",
        "#  • Parallelized computation for surrogate data analysis to ensure tractable\n",
        "#    execution times.\n",
        "#  • A modular, end-to-end orchestrator that manages the entire workflow from\n",
        "#    raw data ingestion to final report generation.\n",
        "#  • Automated quality control, benchmark verification, and statistical\n",
        "#    significance testing to ensure the robustness and credibility of results.\n",
        "#  • Publication-quality output generation for both tabular summaries and\n",
        "#    scientific visualizations.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Masoudi, O., Shahbazi, F., & Sharifi, M. (2025). Complexity of Financial\n",
        "#  Time Series: Multifractal and Multiscale Entropy Analyses.\n",
        "#  arXiv preprint arXiv:2507.23414.\n",
        "#  https://arxiv.org/abs/2507.23414\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import itertools\n",
        "import copy\n",
        "from datetime import datetime\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "\n",
        "# --- Third-Party Library Imports ---\n",
        "# Core data manipulation and numerical computation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Statistical functions and tests\n",
        "from scipy import stats\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging for status updates\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
      ],
      "metadata": {
        "id": "oJciUF_NleJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "JFSZqWB3lgpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Inputs, Processes and Output (IPO) Analysis of All Callables\n",
        "\n",
        "#### **Module 1: Data Validation and Preparation**\n",
        "\n",
        "**1. Callable: `_validate_study_config`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `study_config`: A `Dict[str, Any]` representing the entire configuration for the analysis.\n",
        "*   **Processes:**\n",
        "    1.  Performs a recursive, schema-based validation of the input dictionary's structure, ensuring all required keys and nested keys are present.\n",
        "    2.  Validates the data type of each configuration value against a predefined schema (e.g., `int`, `float`, `np.ndarray`).\n",
        "    3.  Executes a series of specific constraint checks on key hyperparameter values, such as ensuring the date format is correct, numerical values are within valid ranges (e.g., `0 < tolerance_r_factor < 1`), and the `q_range` does not contain zero.\n",
        "    4.  If any check fails, it raises a `KeyError`, `TypeError`, or `ValueError` with a precise, informative message indicating the exact location of the failure.\n",
        "*   **Outputs:**\n",
        "    *   `None`: The function returns `None` upon successful validation, serving as a procedural gatekeeper.\n",
        "*   **Data Transformation:** This function is a pure validator; it performs no data transformation. It reads the input dictionary and either allows the program to proceed or halts it with an exception.\n",
        "*   **Role in Research Pipeline:** This callable serves as the foundational **gatekeeper for methodological fidelity**. It ensures that the entire analysis is executed with the exact hyperparameters and structural settings intended by the researcher, preventing silent failures or scientifically invalid results due to misconfiguration. It enforces the specific parameters mentioned throughout the paper, such as the `embedding_dimension_m = 3` and `tolerance_r_factor = 0.15` cited in Section 4.2.\n",
        "\n",
        "**2. Callable: `_validate_dataframe_quality` **\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_prices`: A `pd.DataFrame` of raw asset prices.\n",
        "    *   `expected_columns`: A `List[str]` of required column names.\n",
        "*   **Processes:**\n",
        "    1.  Conducts a comprehensive battery of structural and quality checks on the input DataFrame.\n",
        "    2.  Validates structural properties: index is a `pd.DatetimeIndex`, columns match expectations, index is monotonic and has no duplicates, and the inferred frequency is Business Day (`'B'`).\n",
        "    3.  Assesses data quality: checks for the presence of any `NaN` values and any non-positive (`<= 0`) prices.\n",
        "    4.  Performs a non-mutating outlier check by calculating log-returns and identifying any daily changes that exceed a 3-sigma threshold based on a 30-day rolling window.\n",
        "    5.  Aggregates all detected issues into a list of error messages.\n",
        "*   **Outputs:**\n",
        "    *   `None`: Returns `None` if all checks pass. Raises a consolidated `ValueError` if any issues are found.\n",
        "*   **Data Transformation:** This is a pure validation function and does not transform the input data. It creates temporary data structures (e.g., log-returns) internally for its checks, but the input `df_prices` remains unmodified.\n",
        "*   **Role in Research Pipeline:** This callable is the **guardian of data integrity**. Before any calculations are performed, it rigorously verifies that the raw input data meets the fundamental assumptions of a clean, well-structured financial time series. This prevents errors in subsequent stages (e.g., taking the logarithm of a non-positive price) and ensures that the analysis is not contaminated by data quality artifacts like gaps, duplicates, or extreme, erroneous price spikes.\n",
        "\n",
        "**3. Callable: `_cleanse_and_truncate_data` **\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_prices`: A `pd.DataFrame` of raw asset prices.\n",
        "    *   `time_series_length`: An `int` specifying the final desired length.\n",
        "    *   `study_end_date`: A `str` specifying the final date of the study period.\n",
        "*   **Processes:**\n",
        "    1.  Creates a deep copy of the input DataFrame to ensure immutability.\n",
        "    2.  Performs a strict, ordered sequence of cleansing operations: sorts the index, removes duplicate dates, and filters out any rows with non-positive prices.\n",
        "    3.  Executes a sophisticated gap-filling protocol: it identifies contiguous blocks of `NaN`s and uses forward-and-backward filling *only* on isolated gaps of fewer than 3 periods, then drops any rows with remaining (larger) gaps.\n",
        "    4.  Implements an outlier removal protocol by calculating log-returns and filtering out any *entire row* where any asset's daily change exceeds a dynamic 3-sigma threshold.\n",
        "    5.  Filters the data to include only dates up to `study_end_date`.\n",
        "    6.  Truncates the resulting clean DataFrame to the exact `time_series_length` by taking the tail.\n",
        "*   **Outputs:**\n",
        "    *   A new, cleansed, and truncated `pd.DataFrame`.\n",
        "*   **Data Transformation:** This function is a pure transformation engine. It takes a raw DataFrame and systematically filters, fills, and slices it, transforming it into a clean, analysis-ready DataFrame of uniform length and high quality.\n",
        "*   **Role in Research Pipeline:** This callable implements the crucial **data preparation** stage. The quality of the entire study depends on this step. It ensures that the time series fed into the complexity algorithms are free from common data issues that could severely bias the results, directly fulfilling the requirement for a uniform data length of N=3,730 as stated in Section 3, \"Data Description\".\n",
        "\n",
        "**4. Callable: `validate_and_prepare_data`**\n",
        "\n",
        "*   **Inputs:** `df_prices` (`pd.DataFrame`), `study_config` (`Dict`), `expected_columns` (`List[str]`).\n",
        "*   **Processes:** Orchestrates the execution of the Task 1 helpers. It calls `_validate_study_config`, then `_validate_dataframe_quality`, and finally `_cleanse_and_truncate_data`.\n",
        "*   **Outputs:** The clean, validated, and truncated `pd.DataFrame` returned by `_cleanse_and_truncate_data`.\n",
        "*   **Data Transformation:** It orchestrates the transformation performed by its helper functions.\n",
        "*   **Role in Research Pipeline:** This is the **master orchestrator for Task 1**, providing a single, clean entry point to the entire data validation and preparation workflow.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 2: Data Preprocessing and Characterization**\n",
        "\n",
        "**5. Callable: `_calculate_log_returns`**\n",
        "\n",
        "*   **Inputs:** `df_prices`: A clean `pd.DataFrame` of asset prices.\n",
        "*   **Processes:**\n",
        "    1.  Validates that all prices are positive.\n",
        "    2.  Applies the natural logarithm to every price in the DataFrame.\n",
        "    3.  Computes the first discrete difference of the logged prices.\n",
        "    4.  Removes the first row, which will contain `NaN` values as a result of the differencing.\n",
        "*   **Outputs:** A new `pd.DataFrame` containing the log-returns, with `N-1` rows.\n",
        "*   **Data Transformation:** It transforms a non-stationary price series into a more stationary log-return series, which represents the continuous compound rate of return.\n",
        "*   **Role in Research Pipeline:** This function implements the fundamental data transformation described in Section 3 and Equation (15). This is the most critical transformation in the entire study, as both the RCMSE and MF-DFA analyses are performed on these log-return series.\n",
        "    $$\n",
        "    \\text{Logarithmic Return} = \\ln(P_t) - \\ln(P_{t-1}) = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right)\n",
        "    $$\n",
        "\n",
        "**6. Callable: `_characterize_log_returns`**\n",
        "\n",
        "*   **Inputs:** `df_log_returns`: A `pd.DataFrame` of log-returns.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each column (asset) of the DataFrame.\n",
        "    2.  For each asset's return series, it computes four key statistical moments: the mean, the sample standard deviation (`ddof=1`), the unbiased sample skewness, and the unbiased sample excess kurtosis (Fisher's definition).\n",
        "    3.  It uses the robust `scipy.stats` library for skewness and kurtosis calculations.\n",
        "*   **Outputs:** A nested `Dict[str, Dict[str, float]]` containing the computed statistics for each asset.\n",
        "*   **Data Transformation:** This function transforms a DataFrame of time series data into a structured dictionary of summary statistics.\n",
        "*   **Role in Research Pipeline:** This callable generates the quantitative results for the **preliminary data analysis**, as presented in Section 4.1 and Table 1. It provides the initial characterization of the return distributions, quantifying their deviation from normality via skewness and kurtosis, which is a key part of the paper's narrative. It implements the concepts behind:\n",
        "    $$\n",
        "    \\text{Skewness} = \\frac{\\mu_3}{\\sigma^3} \\quad (\\text{Equation 16})\n",
        "    $$\n",
        "    $$\n",
        "    \\text{ex-Kurtosis} = \\frac{\\mu_4}{\\sigma^4} - 3 \\quad (\\text{Equation 17})\n",
        "    $$\n",
        "\n",
        "**7. Callable: `_generate_shuffled_control_set`**\n",
        "\n",
        "*   **Inputs:** `df_log_returns` (`pd.DataFrame`), `seed` (`int`).\n",
        "*   **Processes:**\n",
        "    1.  Creates a deep copy of the input DataFrame.\n",
        "    2.  Initializes a seeded random number generator for reproducibility.\n",
        "    3.  Iterates through each column of the copied DataFrame.\n",
        "    4.  For each column, it randomly permutes the temporal order of its values, leaving the index and other columns untouched.\n",
        "*   **Outputs:** A new `pd.DataFrame` with the same shape, index, and columns as the input, but with shuffled values in each column.\n",
        "*   **Data Transformation:** It transforms a temporally structured time series into a temporally random one while perfectly preserving its value distribution.\n",
        "*   **Role in Research Pipeline:** This function creates the **null-hypothesis dataset** required for the control analysis. As discussed throughout the paper (e.g., in the analysis of Figure 7 and Figure 10), comparing the results from the original data to the results from this shuffled data is the method used to determine if the observed complexity is due to meaningful temporal correlations or simply the shape of the data's distribution.\n",
        "\n",
        "**8. Callable: `preprocess_and_characterize_data`**\n",
        "\n",
        "*   **Inputs:** `df_clean_prices` (`pd.DataFrame`), `seed` (`int`).\n",
        "*   **Processes:** Orchestrates the execution of the Task 2 helpers. It calls `_calculate_log_returns`, `_characterize_log_returns`, and `_generate_shuffled_control_set`.\n",
        "*   **Outputs:** A `Tuple` containing the `df_log_returns`, `df_log_returns_shuffled`, and the `statistical_properties` dictionary.\n",
        "*   **Data Transformation:** It orchestrates the transformation from clean prices to the three core analytical inputs.\n",
        "*   **Role in Research Pipeline:** This is the **master orchestrator for Task 2**, providing a single entry point to generate all the necessary data structures for the main complexity analyses.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 3: RCMSE Implementation Framework**\n",
        "\n",
        "**9. Callable: `_compute_sampen_components` **\n",
        "\n",
        "*   **Inputs:** `series` (`np.ndarray`), `m` (`int`), `r` (`float`).\n",
        "*   **Processes:**\n",
        "    1.  Creates two memory-efficient \"views\" of the input series using `numpy.lib.stride_tricks.as_strided`: one for embedding vectors of length `m`, and one for length `m+1`.\n",
        "    2.  For each embedding dimension, it calculates the Chebyshev distance between all pairs of vectors in a single, vectorized operation using NumPy broadcasting.\n",
        "    3.  It counts the number of vector pairs whose distance is less than or equal to the tolerance `r`, excluding self-matches.\n",
        "*   **Outputs:** A `Tuple[int, int]` containing `n_m` (the count of matching pairs of length `m`) and `n_m_plus_1` (the count of matching pairs of length `m+1`).\n",
        "*   **Data Transformation:** It transforms a 1D time series into two scalar integer counts representing the frequency of repeating patterns.\n",
        "*   **Role in Research Pipeline:** This is the high-performance **computational core of the Sample Entropy algorithm**. It implements the pattern-matching logic that underpins the entire RCMSE analysis, specifically the counting of vectors that satisfy the distance condition of Equation (2):\n",
        "    $$\n",
        "    d[X_m(i), X_m(j)] = \\max_{0 \\le k \\le m-1} \\{|x_{i+k} - x_{j+k}|\\} \\le r\n",
        "    $$\n",
        "\n",
        "**10. Callable: `compute_rcmse_profile`**\n",
        "\n",
        "*   **Inputs:** `series` (`np.ndarray`), `rcmse_config` (`Dict`).\n",
        "*   **Processes:**\n",
        "    1.  Calculates a single, fixed tolerance `r` based on the standard deviation of the original series.\n",
        "    2.  Iterates through each time scale `τ` from 1 to `max_scale_tau`.\n",
        "    3.  For `τ=1`, it calls `_compute_sampen_components` on the original series.\n",
        "    4.  For `τ>1`, it generates `τ` overlapping coarse-grained series according to the refined composite method.\n",
        "    5.  For each coarse-grained series, it calls `_compute_sampen_components` and aggregates the resulting `n_m` and `n_m_plus_1` counts.\n",
        "    6.  After processing all series for a given `τ`, it calculates the final RCMSE value using the logarithm of the ratio of the *total aggregated counts*.\n",
        "    7.  It calculates the total complexity by summing the RCMSE values across all scales.\n",
        "*   **Outputs:** A `Dict` containing the `rcmse_profile` (a 1D `np.ndarray` of entropy values) and the `total_complexity` (a `float`).\n",
        "*   **Data Transformation:** It transforms a 1D time series into a complexity profile (entropy vs. scale) and a single scalar complexity score.\n",
        "*   **Role in Research Pipeline:** This function implements the **full Refined Composite Multiscale Sample Entropy (RCMSE) analysis**, generating the results shown in Figure 6 and Table 2. It is the direct implementation of the multiscale entropy methodology, including the coarse-graining procedure of Equation (4) (conceptually) and the final aggregation and calculation of Equation (6):\n",
        "    $$\n",
        "    \\text{RCMSE}(x, \\tau, m, r) = -\\ln\\left(\\frac{\\sum_{k=1}^{\\tau} n^{m+1}_{(k,\\tau)}}{\\sum_{k=1}^{\\tau} n^{m}_{(k,\\tau)}}\\right)\n",
        "    $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 4: MF-DFA Implementation Framework**\n",
        "\n",
        "**11. Callable: `_construct_mdfa_profile`**\n",
        "\n",
        "*   **Inputs:** `series` (`np.ndarray`).\n",
        "*   **Processes:** Centers the input series by subtracting its mean, then computes the cumulative sum.\n",
        "*   **Outputs:** A 1D `np.ndarray` representing the profile `Y(i)`.\n",
        "*   **Data Transformation:** Transforms a stationary-like series into a non-stationary, random-walk-like profile.\n",
        "*   **Role in Research Pipeline:** Implements **Step 1 of the MF-DFA algorithm**, as defined by Equation (7):\n",
        "    $$\n",
        "    Y(i) = \\sum_{k=1}^{i} (x_k - \\langle x \\rangle)\n",
        "    $$\n",
        "\n",
        "**12. Callable: `_compute_fluctuation_function`**\n",
        "\n",
        "*   **Inputs:** `profile` (`np.ndarray`), `s_range` (`np.ndarray`), `q_range` (`np.ndarray`), `polynomial_order` (`int`).\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each scale `s` in `s_range`.\n",
        "    2.  For each `s`, it divides the profile into `2*N_s` segments using the bidirectional approach.\n",
        "    3.  For each segment, it fits and subtracts a polynomial of the specified order to find the local trend.\n",
        "    4.  It calculates the variance of the residual (the detrended segment).\n",
        "    5.  It then iterates through each moment `q` in `q_range` and computes the `q`-th order average of the variances to get `F_q(s)`. It handles the `q=0` case with a geometric mean.\n",
        "*   **Outputs:** A 2D `np.ndarray` of shape `(len(q_range), len(s_range))` containing the `F_q(s)` values.\n",
        "*   **Data Transformation:** Transforms the profile series into a matrix of fluctuation functions, which quantifies how the average fluctuation magnitude scales with both the observation window size `s` and the statistical moment `q`.\n",
        "*   **Role in Research Pipeline:** This is the **computational core of the MF-DFA**, implementing Steps 2, 3, and 4 of the algorithm. It calculates the variance `F^2(s,v)` from Equations (8) & (9) and then the final fluctuation function `F_q(s)` from Equation (10):\n",
        "    $$\n",
        "    F_q(s) = \\left[ \\frac{1}{2N_s} \\sum_{v=1}^{2N_s} [F^2(s,v)]^{q/2} \\right]^{1/q}\n",
        "    $$\n",
        "\n",
        "**13. Callable: `_extract_scaling_exponents`**\n",
        "\n",
        "*   **Inputs:** `f_q_s` (2D `np.ndarray`), `s_range` (`np.ndarray`).\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each row of the `f_q_s` matrix (each corresponding to a value of `q`).\n",
        "    2.  For each `q`, it performs a linear regression of `log(F_q(s))` against `log(s)`.\n",
        "    3.  It extracts the slope of the regression line.\n",
        "*   **Outputs:** A 1D `np.ndarray` containing the generalized Hurst exponents `h(q)`.\n",
        "*   **Data Transformation:** It transforms the matrix of fluctuation functions into a single vector of scaling exponents.\n",
        "*   **Role in Research Pipeline:** This function implements **Step 5 of the MF-DFA algorithm**, determining the scaling behavior of the time series. It finds the exponent `h(q)` in the power-law relationship defined by Equation (11):\n",
        "    $$\n",
        "    F_q(s) \\sim s^{h(q)}\n",
        "    $$\n",
        "\n",
        "**14. Callable: `_compute_singularity_spectrum`**\n",
        "\n",
        "*   **Inputs:** `h_q` (`np.ndarray`), `q_range` (`np.ndarray`).\n",
        "*   **Processes:**\n",
        "    1.  Calculates the mass exponent `τ(q)` from `h(q)` and `q`.\n",
        "    2.  Numerically differentiates `τ(q)` with respect to `q` to find the Hölder exponent `α(q)`.\n",
        "    3.  Performs a Legendre transform on `τ(q)` to find the singularity spectrum `f(α)`.\n",
        "    4.  Calculates the width of the spectrum, `Δα`.\n",
        "*   **Outputs:** A `Dict` containing `tau_q`, `alpha`, `f_alpha`, and `delta_alpha`.\n",
        "*   **Data Transformation:** It transforms the vector of scaling exponents `h(q)` into the final multifractal spectrum, which is the primary \"fingerprint\" of the system's complexity.\n",
        "*   **Role in Research Pipeline:** This function performs the final calculations of the MF-DFA, generating the results needed to plot the **singularity spectrum** (Figure 8) and populate Table 3. It is the direct implementation of the Legendre transform defined by Equations (12) and (13):\n",
        "    $$\n",
        "    \\tau(q) = qh(q) - 1\n",
        "    $$\n",
        "    $$\n",
        "    \\alpha(q) = \\frac{d\\tau(q)}{dq}, \\quad f(\\alpha) = q\\alpha - \\tau(q)\n",
        "    $$\n",
        "\n",
        "**15. Callable: `compute_mfdfa_analysis`**\n",
        "\n",
        "*   **Inputs:** `series` (`np.ndarray`), `mdfa_config` (`Dict`).\n",
        "*   **Processes:** Orchestrates the entire MF-DFA workflow by calling `_construct_mdfa_profile`, `_compute_fluctuation_function`, `_extract_scaling_exponents`, and `_compute_singularity_spectrum` in sequence.\n",
        "*   **Outputs:** A comprehensive `Dict` containing all intermediate and final results of the MF-DFA.\n",
        "*   **Data Transformation:** It orchestrates the transformation of a 1D time series into a full multifractal characterization.\n",
        "*   **Role in Research Pipeline:** This is the **master orchestrator for Task 4**, providing a single entry point to the entire MF-DFA analysis.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 5-8: Analysis, Reporting, and Orchestration**\n",
        "\n",
        "**16. Callable: `conduct_shuffling_analysis` **\n",
        "\n",
        "*   **Inputs:** `df_log_returns` (`pd.DataFrame`), `original_results` (`Dict`), `study_config` (`Dict`), etc.\n",
        "*   **Processes:**\n",
        "    1.  Calls `_generate_shuffled_datasets` to create the surrogate ensemble.\n",
        "    2.  Calls `_compute_shuffled_metrics` to compute complexity metrics for the ensemble in parallel.\n",
        "    3.  Calculates summary statistics (mean, std, z-score, nonlinearity index) by comparing the original metrics to the distribution of shuffled metrics.\n",
        "*   **Outputs:** A `Tuple` containing two dictionaries: one with the summary statistics and one with the raw lists of shuffled metrics.\n",
        "*   **Data Transformation:** Transforms the original and surrogate time series data into a quantitative comparison of their complexity.\n",
        "*   **Role in Research Pipeline:** This function executes the **entire control analysis**. It provides the statistical evidence needed to argue that the complexity observed in the original series is due to its temporal structure, a central conclusion of the paper.\n",
        "\n",
        "**17. Callable: `run_complexity_analysis_pipeline` **\n",
        "\n",
        "*   **Inputs:** `df_prices` (`pd.DataFrame`), `study_config` (`Dict`).\n",
        "*   **Processes:** Serves as the top-level orchestrator, sequentially calling the master functions for Task 1, Task 2, Tasks 3 & 4 (per asset), and Task 5. It manages the entire data flow and aggregates all results.\n",
        "*   **Outputs:** A single, comprehensive, nested `Dict` containing all results from all stages of the analysis, including the raw and summary shuffling results.\n",
        "*   **Data Transformation:** Orchestrates the transformation of a raw price DataFrame into the final, all-encompassing results object.\n",
        "*   **Role in Research Pipeline:** This is the **main entry point for the entire scientific study**, encapsulating the full research methodology in a single, reproducible function call.\n",
        "\n",
        "**18. Callable: `conduct_statistical_significance_tests` **\n",
        "\n",
        "*   **Inputs:** `shuffling_summary` (`Dict`), `shuffling_raw` (`Dict`), `alpha` (`float`).\n",
        "*   **Processes:**\n",
        "    1.  Calculates the Bonferroni-corrected significance threshold.\n",
        "    2.  For each asset and metric, it computes an empirical one-sided p-value by comparing the original metric to the distribution of shuffled metrics.\n",
        "    3.  It determines if the result is statistically significant based on the corrected alpha.\n",
        "    4.  It calculates a 95% bootstrap confidence interval for the mean of the shuffled distribution.\n",
        "*   **Outputs:** A `Dict` containing a detailed statistical report with p-values, significance flags, and confidence intervals.\n",
        "*   **Data Transformation:** Transforms the raw distributions of shuffled metrics into a formal statistical hypothesis test report.\n",
        "*   **Role in Research Pipeline:** This function provides the **formal statistical validation** for the shuffling analysis. It moves beyond the visual and descriptive comparisons to provide rigorous, quantitative evidence (p-values) for the paper's conclusions about the presence of nonlinear correlations.\n",
        "\n",
        "**19. Callable: `generate_summary_tables`**\n",
        "\n",
        "*   **Inputs:** `analysis_results` (`Dict`).\n",
        "*   **Processes:** Navigates the nested results dictionary, extracts the specific scalar values needed for each of the paper's main tables, and formats them into human-readable `pd.DataFrame` objects.\n",
        "*   **Outputs:** A `Dict` where keys are table names and values are the corresponding `pd.DataFrame` objects.\n",
        "*   **Data Transformation:** Transforms the computational results dictionary into a set of presentation-ready tables.\n",
        "*   **Role in Research Pipeline:** This is the **primary reporting tool for quantitative results**, directly generating the tabular summaries (Tables 1-4) that form the core of the paper's findings.\n",
        "\n",
        "**20. Callable: `generate_publication_visualizations`**\n",
        "\n",
        "*   **Inputs:** `analysis_results` (`Dict`), `output_dir` (`str`).\n",
        "*   **Processes:** Uses `matplotlib` and `seaborn` to create plots that visually represent the key findings, such as the RCMSE profiles and singularity spectra, and saves them as image files.\n",
        "*   **Outputs:** A `Dict` mapping figure names to the file paths where they were saved.\n",
        "*   **Data Transformation:** Transforms numerical array data from the results dictionary into graphical representations.\n",
        "*   **Role in Research Pipeline:** This is the **primary reporting tool for visual results**, generating the figures (e.g., Figures 6, 8) that provide an intuitive understanding of the data's complex behavior.\n",
        "\n",
        "**21. Callable: `verify_results_against_benchmarks` **\n",
        "\n",
        "*   **Inputs:** `analysis_results` (`Dict`), `benchmarks` (`Dict`).\n",
        "*   **Processes:**\n",
        "    1.  Iterates through a predefined set of benchmark checks.\n",
        "    2.  For each check, it safely traverses the nested results dictionary using a robust path tuple to retrieve the actual computed value.\n",
        "    3.  It compares the actual value to the expected benchmark value using a relative tolerance.\n",
        "*   **Outputs:** A `Dict` containing a detailed verification report with the pass/fail status of each check.\n",
        "*   **Data Transformation:** Transforms the results dictionary and a set of benchmarks into a validation report.\n",
        "*   **Role in Research Pipeline:** This function serves as the **final automated quality assurance and replication check**. It programmatically verifies that the implementation produces results consistent with those reported in the source paper, providing a high degree of confidence in the correctness of the entire framework.\n",
        "\n",
        "**22. Callable: `market_complexity_dashboard_generator` **\n",
        "\n",
        "*   **Inputs:** `df_prices` (`pd.DataFrame`), `study_config` (`Dict`), `run_sensitivity` (`bool`).\n",
        "*   **Processes:** Acts as the ultimate master orchestrator. It calls the main analysis pipeline, the optional sensitivity analysis, and all the final reporting and verification functions. It also handles the serialization of the final results to a JSON file using a custom encoder.\n",
        "*   **Outputs:** A master `Dict` containing all artifacts produced during the run.\n",
        "*   **Data Transformation:** Orchestrates all data transformations in the entire project.\n",
        "*   **Role in Research Pipeline:** This is the **apex callable**, providing a single, all-encompassing interface to the entire research project, from raw data to final, verified, and persisted reports.\n",
        "\n",
        "\n",
        "### Usage Example\n",
        "\n",
        "\n",
        "The following is a granular, step-by-step guide on how to use the end-to-end pipeline via its master orchestrator, `market_complexity_dashboard_generator`. This example will serve as a template for conducting a complete and rigorous market complexity analysis, from initial data loading to the final interpretation of the generated reports.\n",
        "\n",
        "--\n",
        "\n",
        "### **Example Usage: End-to-End Market Complexity Analysis**\n",
        "\n",
        "This example demonstrates the full workflow for analyzing the four assets mentioned in the paper: Bitcoin, GBP/USD, Gold, and Natural Gas.\n",
        "\n",
        "#### **Step 1: Setup and Data Acquisition**\n",
        "\n",
        "The first step is to prepare the environment by importing the necessary libraries and the framework itself. We then acquire the raw data. For a professional application, this data would come from a dedicated provider; for this demonstration, we will simulate this by creating a sample `pandas.DataFrame`.\n",
        "\n",
        "```python\n",
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import the complete, remediated framework from its module\n",
        "# (Assuming all functions are saved in a file named 'market_complexity_framework.py')\n",
        "# from market_complexity_framework import market_complexity_dashboard_generator\n",
        "\n",
        "# --- Data Acquisition ---\n",
        "# In a real-world scenario, this data would be loaded from a CSV, a database,\n",
        "# or a financial data API like Bloomberg or Refinitiv.\n",
        "# For this example, we will generate a synthetic DataFrame that mimics the\n",
        "# structure of the required input data.\n",
        "\n",
        "# Define the date range and asset columns.\n",
        "dates = pd.bdate_range(end='2024-12-02', periods=3730)\n",
        "assets = ['BTC-USD', 'GBP-USD', 'GOLD', 'NATGAS']\n",
        "\n",
        "# Generate synthetic price data: a geometric random walk.\n",
        "# This ensures prices are always positive and have some realistic structure.\n",
        "np.random.seed(42) # for reproducibility\n",
        "initial_prices = [10000, 1.3, 1800, 3.5]\n",
        "daily_vol = [0.04, 0.005, 0.01, 0.03]\n",
        "log_returns = {\n",
        "    asset: np.random.normal(0, vol, 3729)\n",
        "    for asset, vol in zip(assets, daily_vol)\n",
        "}\n",
        "\n",
        "# Create the price DataFrame from the log returns.\n",
        "df_prices = pd.DataFrame(index=dates)\n",
        "for i, asset in enumerate(assets):\n",
        "    # Calculate cumulative log returns and exponentiate to get the price path\n",
        "    price_path = initial_prices[i] * np.exp(np.cumsum(np.insert(log_returns[asset], 0, 0)))\n",
        "    df_prices[asset] = price_path\n",
        "\n",
        "print(\"Sample of generated raw price data:\")\n",
        "print(df_prices.head())\n",
        "print(\"\\nShape of the raw price data:\", df_prices.shape)\n",
        "```\n",
        "\n",
        "#### **Step 2: Define the Study Configuration**\n",
        "\n",
        "The entire analysis is controlled by a single, comprehensive configuration dictionary. This is where all hyperparameters and global settings are defined. Using a dictionary for configuration is a critical best practice for ensuring reproducibility and making the analysis easy to modify. We will use the exact parameters specified in the original paper.\n",
        "\n",
        "```python\n",
        "# --- Study Configuration ---\n",
        "# This dictionary defines every parameter for the analysis, ensuring that the\n",
        "# entire study is reproducible from a single configuration object.\n",
        "\n",
        "study_config = {\n",
        "    \"global_parameters\": {\n",
        "        \"study_end_date\": \"2024-12-02\",\n",
        "        \"time_series_length\": 3730,\n",
        "    },\n",
        "    \"analytical_modules\": {\n",
        "        \"rcmse_analysis\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"embedding_dimension_m\": 3,\n",
        "                \"tolerance_r_factor\": 0.15,\n",
        "                \"max_scale_tau\": 100,\n",
        "            }\n",
        "        },\n",
        "        \"mdfa_analysis\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"q_range\": np.array(\n",
        "                    [-5.0, -4.0, -3.0, -2.0, -1.0, -0.1, 0.1, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
        "                ), # A smaller q_range for a faster example run.\n",
        "                \"s_range_generation_params\": {\n",
        "                    \"s_min\": 10,\n",
        "                    \"s_max_divisor\": 4,\n",
        "                    \"num_points\": 30,\n",
        "                },\n",
        "                \"polynomial_order\": 1,\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Step 3: Execute the End-to-End Pipeline**\n",
        "\n",
        "With the data and configuration prepared, the entire analysis is executed with a single call to the master orchestrator function, `market_complexity_dashboard_generator`. This function encapsulates all tasks: validation, cleansing, preprocessing, core complexity analysis, shuffling analysis, reporting, and verification.\n",
        "\n",
        "```python\n",
        "# --- Pipeline Execution ---\n",
        "# This single function call runs the entire end-to-end analysis.\n",
        "# It ingests the raw data and the configuration, and returns a comprehensive\n",
        "# dictionary containing all results, reports, tables, and figure paths.\n",
        "\n",
        "# For this demonstration, we will set run_sensitivity to False to avoid the\n",
        "# lengthy computation, but it could be set to True for a full robustness check.\n",
        "final_dashboard_results = market_complexity_dashboard_generator(\n",
        "    df_prices=df_prices,\n",
        "    study_config=study_config,\n",
        "    run_sensitivity=False\n",
        ")\n",
        "```\n",
        "\n",
        "#### **Step 4: Review and Interpret the Outputs**\n",
        "\n",
        "The `final_dashboard_results` object is a structured dictionary containing all artifacts from the study. The final step is to programmatically access and review these outputs.\n",
        "\n",
        "##### **4.1 Reviewing Summary Tables**\n",
        "\n",
        "The generated tables provide a high-level quantitative summary of the key findings, formatted for direct interpretation.\n",
        "\n",
        "```python\n",
        "# --- Reviewing Tabular Results ---\n",
        "# The 'summary_tables' key holds a dictionary of pandas DataFrames.\n",
        "\n",
        "print(\"\\n--- Summary Tables ---\")\n",
        "\n",
        "# Display Table 1: Statistical Properties\n",
        "print(\"\\nTable 1: Statistical Properties\")\n",
        "print(final_dashboard_results['summary_tables']['Table1_Statistical_Properties'].round(3))\n",
        "\n",
        "# Display Table 2: RCMSE Complexity\n",
        "print(\"\\nTable 2: RCMSE Complexity\")\n",
        "print(final_dashboard_results['summary_tables']['Table2_RCMSE_Complexity'].round(3))\n",
        "\n",
        "# Display Table 3: MF-DFA Spectrum Widths\n",
        "print(\"\\nTable 3: MF-DFA Spectrum Widths\")\n",
        "print(final_dashboard_results['summary_tables']['Table3_MFDFA_Spectrum_Widths'].round(3))\n",
        "\n",
        "# Display Table 4: Hurst Exponents\n",
        "print(\"\\nTable 4: Hurst Exponents\")\n",
        "print(final_dashboard_results['summary_tables']['Table4_Hurst_Exponents'].round(3))\n",
        "```\n",
        "\n",
        "##### **4.2 Reviewing Validation and Verification Reports**\n",
        "\n",
        "Before trusting the results, it is imperative to check the automated validation and verification reports.\n",
        "\n",
        "```python\n",
        "# --- Reviewing Validation and Verification ---\n",
        "print(\"\\n--- Validation and Verification Reports ---\")\n",
        "\n",
        "# Check the pipeline validation report for any data or methodological issues.\n",
        "pipeline_validation_status = final_dashboard_results['pipeline_validation_report']['summary']['overall_status']\n",
        "print(f\"\\nPipeline Validation Status: {pipeline_validation_status}\")\n",
        "if pipeline_validation_status == 'FAIL':\n",
        "    print(\"Errors:\", final_dashboard_results['pipeline_validation_report']['summary']['errors'])\n",
        "\n",
        "# Check the benchmark verification report to ensure our results align with known values.\n",
        "# Note: This will likely fail with synthetic data but demonstrates the process.\n",
        "benchmark_report = final_dashboard_results['benchmark_verification_report']\n",
        "print(f\"\\nBenchmark Verification Status: {benchmark_report['summary']['overall_status']}\")\n",
        "print(\"Details:\")\n",
        "for check in benchmark_report['details']:\n",
        "    print(f\"  - Check: {check['check']}, Status: {check['status']}\")\n",
        "\n",
        "```\n",
        "\n",
        "##### **4.3 Accessing Deeper Results**\n",
        "\n",
        "The raw numerical results are available for more granular analysis, such as examining the full shuffling distributions or the RCMSE profile of a specific asset.\n",
        "\n",
        "```python\n",
        "# --- Accessing Granular Results ---\n",
        "print(\"\\n--- Granular Result Inspection ---\")\n",
        "\n",
        "# Example: Inspect the shuffling analysis summary for Bitcoin.\n",
        "btc_shuffling_summary = final_dashboard_results['main_analysis_results']['shuffling_analysis_summary']['BTC-USD']\n",
        "delta_alpha_z_score = btc_shuffling_summary['delta_alpha']['z_score']\n",
        "print(f\"\\nZ-score for BTC-USD delta_alpha (original vs. shuffled): {delta_alpha_z_score:.2f}\")\n",
        "print(\"A large positive z-score indicates the original data's multifractality is significantly greater than random noise.\")\n",
        "\n",
        "# Example: Access the full RCMSE profile for Gold.\n",
        "gold_rcmse_profile = final_dashboard_results['main_analysis_results']['GOLD']['rcmse']['rcmse_profile']\n",
        "print(f\"\\nFirst 5 RCMSE values for GOLD: {np.round(gold_rcmse_profile[:5], 3)}\")\n",
        "```\n",
        "\n",
        "This example provides a complete, professional, and reproducible workflow. It demonstrates how the modular, orchestrated framework allows a researcher to move systematically from raw data to a deep, multi-faceted, and verifiable understanding of market complexity with clarity and rigor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fa3KQjJ0liMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Input Validation and Data Quality Assurance\n",
        "\n",
        "def _validate_study_config(\n",
        "    study_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the structure and values of the study configuration dictionary.\n",
        "\n",
        "    This function performs a deep validation of the configuration dictionary,\n",
        "    ensuring all required keys, data types, and value constraints are met\n",
        "    according to the research paper's specifications. It raises specific\n",
        "    TypeErrors or ValueErrors if any part of the configuration is invalid.\n",
        "\n",
        "    Args:\n",
        "        study_config (Dict[str, Any]): The configuration dictionary for the study.\n",
        "\n",
        "    Returns:\n",
        "        None: The function returns None if validation is successful.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a required key or nested key is missing.\n",
        "        TypeError: If a value has an incorrect data type.\n",
        "        ValueError: If a value is outside its allowed range or format.\n",
        "    \"\"\"\n",
        "    # Define the validation schema\n",
        "    schema = {\n",
        "        \"global_parameters\": {\n",
        "            \"study_end_date\": str,\n",
        "            \"time_series_length\": int,\n",
        "        },\n",
        "        \"analytical_modules\": {\n",
        "            \"rcmse_analysis\": {\n",
        "                \"hyperparameters\": {\n",
        "                    \"embedding_dimension_m\": int,\n",
        "                    \"tolerance_r_factor\": float,\n",
        "                    \"max_scale_tau\": int,\n",
        "                }\n",
        "            },\n",
        "            \"mdfa_analysis\": {\n",
        "                \"hyperparameters\": {\n",
        "                    \"q_range\": np.ndarray,\n",
        "                    \"s_range_generation_params\": {\n",
        "                        \"s_min\": int,\n",
        "                        \"s_max_divisor\": int,\n",
        "                        \"num_points\": int,\n",
        "                    },\n",
        "                    \"polynomial_order\": int,\n",
        "                }\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Helper function for recursive validation\n",
        "    def _recursive_validate(config_level, schema_level, path=\"\"):\n",
        "        # Check for missing keys in the config\n",
        "        if not isinstance(config_level, dict):\n",
        "            raise TypeError(f\"Configuration path '{path}' must be a dictionary.\")\n",
        "\n",
        "        # Iterate through schema keys to ensure they exist in the config\n",
        "        for key, expected_type in schema_level.items():\n",
        "            # Build the current path for error messaging\n",
        "            current_path = f\"{path}.{key}\" if path else key\n",
        "\n",
        "            # Check if the key exists in the configuration\n",
        "            if key not in config_level:\n",
        "                raise KeyError(f\"Missing required key in configuration: '{current_path}'\")\n",
        "\n",
        "            # Get the value from the configuration\n",
        "            value = config_level[key]\n",
        "\n",
        "            # If the expected type is a dictionary, recurse\n",
        "            if isinstance(expected_type, dict):\n",
        "                _recursive_validate(value, expected_type, current_path)\n",
        "            # Otherwise, validate the type of the value\n",
        "            elif not isinstance(value, expected_type):\n",
        "                raise TypeError(\n",
        "                    f\"Invalid type for '{current_path}'. \"\n",
        "                    f\"Expected {expected_type.__name__}, but got {type(value).__name__}.\"\n",
        "                )\n",
        "\n",
        "    # Perform the recursive validation\n",
        "    _recursive_validate(study_config, schema)\n",
        "\n",
        "    # --- Perform specific value constraint checks after structure validation ---\n",
        "\n",
        "    # Validate global_parameters\n",
        "    gp = study_config[\"global_parameters\"]\n",
        "    try:\n",
        "        # Validate study_end_date format\n",
        "        datetime.strptime(gp[\"study_end_date\"], \"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\n",
        "            \"Invalid format for 'study_end_date'. Expected 'YYYY-MM-DD'.\"\n",
        "        )\n",
        "\n",
        "    # Validate time_series_length\n",
        "    if not gp[\"time_series_length\"] >= 1000:\n",
        "        raise ValueError(\n",
        "            \"'time_series_length' must be a positive integer >= 1000.\"\n",
        "        )\n",
        "\n",
        "    # Validate RCMSE hyperparameters\n",
        "    rcmse_hp = study_config[\"analytical_modules\"][\"rcmse_analysis\"][\"hyperparameters\"]\n",
        "    m = rcmse_hp[\"embedding_dimension_m\"]\n",
        "    if not m >= 2:\n",
        "        raise ValueError(\"'embedding_dimension_m' must be an integer >= 2.\")\n",
        "\n",
        "    # Validate tolerance_r_factor\n",
        "    r_factor = rcmse_hp[\"tolerance_r_factor\"]\n",
        "    if not (0 < r_factor < 1):\n",
        "        raise ValueError(\"'tolerance_r_factor' must be a float in the range (0, 1).\")\n",
        "\n",
        "    # Validate max_scale_tau\n",
        "    if not rcmse_hp[\"max_scale_tau\"] > 0:\n",
        "        raise ValueError(\"'max_scale_tau' must be a positive integer.\")\n",
        "\n",
        "    # Validate MF-DFA hyperparameters\n",
        "    mdfa_hp = study_config[\"analytical_modules\"][\"mdfa_analysis\"][\"hyperparameters\"]\n",
        "    q_range = mdfa_hp[\"q_range\"]\n",
        "    if 0 in q_range:\n",
        "        raise ValueError(\"'q_range' in MF-DFA parameters must not contain zero.\")\n",
        "\n",
        "    # Validate s_range_generation_params\n",
        "    s_params = mdfa_hp[\"s_range_generation_params\"]\n",
        "    for key, val in s_params.items():\n",
        "        if not (isinstance(val, int) and val > 0):\n",
        "            raise ValueError(f\"'s_range_generation_params.{key}' must be a positive integer.\")\n",
        "\n",
        "    # Validate polynomial_order\n",
        "    if not (isinstance(mdfa_hp[\"polynomial_order\"], int) and mdfa_hp[\"polynomial_order\"] >= 0):\n",
        "        raise ValueError(\"'polynomial_order' must be a non-negative integer.\")\n",
        "\n",
        "\n",
        "def _validate_dataframe_quality(\n",
        "    df_prices: pd.DataFrame,\n",
        "    expected_columns: List[str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive quality and structure check on the input DataFrame.\n",
        "\n",
        "    This function validates that the DataFrame meets all structural and quality\n",
        "    prerequisites for the analysis. It now includes previously omitted checks for\n",
        "    business day frequency and the presence of outliers in daily returns, in\n",
        "    addition to the original checks for index type, column names, data integrity,\n",
        "    and chronological order. It aggregates all errors and raises a single,\n",
        "    detailed ValueError if any checks fail.\n",
        "\n",
        "    Args:\n",
        "        df_prices (pd.DataFrame): The input DataFrame of daily closing prices.\n",
        "        expected_columns (List[str]): A list of column names expected to be in\n",
        "                                      the DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        None: Returns None if all quality checks pass.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If any of the structural or quality checks fail, containing\n",
        "                    a consolidated report of all issues found.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect all identified error messages\n",
        "    error_messages = []\n",
        "\n",
        "    # --- Prerequisite Check: Input Type ---\n",
        "    # Ensure the primary input is a pandas DataFrame before proceeding.\n",
        "    if not isinstance(df_prices, pd.DataFrame):\n",
        "        # Raise TypeError immediately as other checks are not possible.\n",
        "        raise TypeError(\"Input 'df_prices' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Step 1.2: DataFrame Structure Validation ---\n",
        "    # Check if the index is a DatetimeIndex, a fundamental requirement.\n",
        "    if not isinstance(df_prices.index, pd.DatetimeIndex):\n",
        "        error_messages.append(\"DataFrame index is not a DatetimeIndex.\")\n",
        "\n",
        "    # Check if the DataFrame contains the exact set of expected columns.\n",
        "    if set(df_prices.columns) != set(expected_columns):\n",
        "        error_messages.append(\n",
        "            f\"DataFrame columns do not match expected. \"\n",
        "            f\"Got {list(df_prices.columns)}, expected {expected_columns}.\"\n",
        "        )\n",
        "\n",
        "    # Check if the index is sorted chronologically.\n",
        "    if not df_prices.index.is_monotonic_increasing:\n",
        "        error_messages.append(\"DataFrame index is not chronologically sorted.\")\n",
        "\n",
        "    # Check for the presence of duplicate dates in the index.\n",
        "    if df_prices.index.duplicated().any():\n",
        "        error_messages.append(\"DataFrame contains duplicate index entries.\")\n",
        "\n",
        "    # Check for Business Day frequency.\n",
        "    inferred_freq = pd.infer_freq(df_prices.index)\n",
        "    if inferred_freq != 'B':\n",
        "        error_messages.append(\n",
        "            f\"Inferred DataFrame frequency is not 'B' (Business Day). \"\n",
        "            f\"Got '{inferred_freq}'.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 1.3: Data Quality Assessment ---\n",
        "    # Check for any missing values (NaNs) in the entire DataFrame.\n",
        "    if df_prices.isnull().values.any():\n",
        "        error_messages.append(\n",
        "            \"DataFrame contains missing values (NaNs). Counts per column:\\n\"\n",
        "            f\"{df_prices.isnull().sum()[df_prices.isnull().sum() > 0]}\"\n",
        "        )\n",
        "\n",
        "    # Check for any non-positive (zero or negative) price values.\n",
        "    if (df_prices <= 0).values.any():\n",
        "        error_messages.append(\n",
        "            \"DataFrame contains non-positive prices. Counts per column:\\n\"\n",
        "            f\"{(df_prices <= 0).sum()[(df_prices <= 0).sum() > 0]}\"\n",
        "        )\n",
        "\n",
        "    # Check for the presence of significant outliers in daily returns.\n",
        "    # This is a non-mutating check to flag potential data quality issues.\n",
        "    if not (df_prices <= 0).values.any(): # Only proceed if log can be taken\n",
        "        # Calculate log-returns to assess price *changes*.\n",
        "        log_returns = np.log(df_prices).diff()\n",
        "        # Define outlier threshold: 3 standard deviations on a 30-day rolling window.\n",
        "        rolling_std = log_returns.rolling(window=30, min_periods=10).std()\n",
        "        outlier_threshold = 3 * rolling_std\n",
        "        # Identify outliers where the absolute return exceeds the threshold.\n",
        "        outliers_mask = log_returns.abs() > outlier_threshold\n",
        "\n",
        "        # Check if any outliers were detected.\n",
        "        if outliers_mask.values.any():\n",
        "            error_messages.append(\n",
        "                \"Potential outliers detected (daily log-return > 3 rolling std). \"\n",
        "                f\"Counts per asset:\\n{outliers_mask.sum()[outliers_mask.sum() > 0]}\"\n",
        "            )\n",
        "\n",
        "    # --- Final Error Reporting ---\n",
        "    # If the error_messages list is not empty, raise a single comprehensive exception.\n",
        "    if error_messages:\n",
        "        # Consolidate all found issues into a single error message.\n",
        "        consolidated_error_message = \"DataFrame validation failed with the following issues:\\n- \" + \"\\n- \".join(error_messages)\n",
        "        raise ValueError(consolidated_error_message)\n",
        "\n",
        "\n",
        "def _cleanse_and_truncate_data(\n",
        "    df_prices: pd.DataFrame,\n",
        "    time_series_length: int,\n",
        "    study_end_date: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans and truncates the price DataFrame using a sophisticated protocol.\n",
        "\n",
        "    This function applies a rigorous, multi-stage data cleansing pipeline:\n",
        "    1.  Sorts the DataFrame chronologically and removes duplicate index entries.\n",
        "    2.  Removes any rows containing non-positive prices.\n",
        "    3.  Performs targeted interpolation (forward fill) ONLY on\n",
        "        isolated gaps of missing data smaller than 3 consecutive periods.\n",
        "    4.  Removes any remaining rows with missing data (i.e., larger gaps).\n",
        "    5.  Removes outlier observations, defined as days where any asset's log-return\n",
        "        exceeds 3 standard deviations from the 30-day rolling mean of absolute\n",
        "        log-returns.\n",
        "    6.  Truncates the final clean data to the specified length ending on or\n",
        "        before the study end date.\n",
        "\n",
        "    Args:\n",
        "        df_prices (pd.DataFrame): The raw, validated price DataFrame.\n",
        "        time_series_length (int): The desired final length of the time series.\n",
        "        study_end_date (str): The final date for the time series ('YYYY-MM-DD').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new, thoroughly cleansed and truncated DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the cleansed data has fewer rows than the required\n",
        "                    'time_series_length'.\n",
        "    \"\"\"\n",
        "    # --- Step 0: Initialization ---\n",
        "    # Create a deep copy to ensure the original DataFrame is not modified.\n",
        "    df_clean = df_prices.copy()\n",
        "\n",
        "    # --- Step 1: Basic Structural Cleansing ---\n",
        "    # Sort DataFrame by index to ensure proper chronological order for subsequent operations.\n",
        "    df_clean = df_clean.sort_index(ascending=True)\n",
        "    # Remove duplicate index entries, keeping the first occurrence.\n",
        "    df_clean = df_clean.loc[~df_clean.index.duplicated(keep='first')]\n",
        "    # Remove any rows with non-positive prices in any column, as log is undefined.\n",
        "    df_clean = df_clean.loc[(df_clean > 0).all(axis=1)]\n",
        "\n",
        "    # --- Step 2: Advanced Gap Filling for Isolated NaNs ---\n",
        "    # This sophisticated logic fills only small, isolated gaps.\n",
        "    for col in df_clean.columns:\n",
        "        # Create a series that assigns a unique ID to each contiguous block of NaNs or non-NaNs.\n",
        "        is_na = df_clean[col].isna()\n",
        "        nan_blocks = is_na.ne(is_na.shift()).cumsum()\n",
        "        # Calculate the size of each of these blocks.\n",
        "        block_sizes = df_clean.groupby(nan_blocks)[col].transform('size')\n",
        "        # Create a boolean mask that is True only for NaNs that are in small gaps (size < 3).\n",
        "        small_gaps_mask = (block_sizes < 3) & is_na\n",
        "        # Apply forward fill ONLY to these identified small gaps.\n",
        "        df_clean.loc[small_gaps_mask, col] = df_clean[col].ffill()\n",
        "\n",
        "    # Remove any rows that still contain NaNs (i.e., gaps of size >= 3).\n",
        "    df_clean.dropna(inplace=True)\n",
        "\n",
        "    # --- Step 3: Outlier Removal ---\n",
        "    # This step removes entire rows where any asset exhibits an extreme price change.\n",
        "    if not df_clean.empty:\n",
        "        # Calculate daily log-returns for the remaining clean data.\n",
        "        log_returns = np.log(df_clean).diff()\n",
        "        # Define the outlier threshold based on a 30-day rolling window.\n",
        "        # The threshold is 3 standard deviations above the rolling mean of *absolute* returns.\n",
        "        rolling_mean_abs = log_returns.abs().rolling(window=30, min_periods=10).mean()\n",
        "        rolling_std_abs = log_returns.abs().rolling(window=30, min_periods=10).std()\n",
        "        outlier_threshold = rolling_mean_abs + 3 * rolling_std_abs\n",
        "        # Create a boolean mask that is True for any row containing at least one outlier.\n",
        "        is_outlier_row = (log_returns.abs() > outlier_threshold).any(axis=1)\n",
        "        # Filter the DataFrame, keeping only the rows that are NOT outliers.\n",
        "        df_clean = df_clean.loc[~is_outlier_row]\n",
        "\n",
        "    # --- Step 4: Final Truncation ---\n",
        "    # Filter data to be on or before the specified study end date.\n",
        "    df_clean = df_clean.loc[df_clean.index <= pd.to_datetime(study_end_date)]\n",
        "\n",
        "    # --- Final Verification ---\n",
        "    # Check if there is enough data remaining after the full cleansing protocol.\n",
        "    if len(df_clean) < time_series_length:\n",
        "        raise ValueError(\n",
        "            f\"Insufficient data after cleansing. Required {time_series_length} \"\n",
        "            f\"data points, but only {len(df_clean)} are available up to \"\n",
        "            f\"{study_end_date}.\"\n",
        "        )\n",
        "\n",
        "    # Select the most recent 'time_series_length' data points to ensure uniform length.\n",
        "    df_clean = df_clean.tail(time_series_length)\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "\n",
        "def validate_and_prepare_data(\n",
        "    df_prices: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    expected_columns: List[str] = [\"BTC_USD\", \"GBP_USD\", \"GC\", \"NG\"]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full data validation and preparation pipeline.\n",
        "\n",
        "    This function serves as the main entry point for Task 1. It validates the\n",
        "    study configuration, checks the quality of the input price DataFrame, and\n",
        "    then applies a rigorous cleansing and truncation protocol.\n",
        "\n",
        "    Args:\n",
        "        df_prices (pd.DataFrame): The raw DataFrame of daily closing prices with\n",
        "                                  a DatetimeIndex.\n",
        "        study_config (Dict[str, Any]): The configuration dictionary for the study.\n",
        "        expected_columns (List[str], optional): A list of required column names.\n",
        "                                                Defaults to the assets from the paper.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A clean, validated, and truncated DataFrame ready for\n",
        "                      the log-return transformation and subsequent analysis.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Validate the entire study configuration dictionary\n",
        "    _validate_study_config(study_config)\n",
        "\n",
        "    # Step 1.2 & 1.3: Perform comprehensive quality checks on the raw DataFrame\n",
        "    _validate_dataframe_quality(df_prices, expected_columns)\n",
        "\n",
        "    # Extract necessary parameters for cleansing\n",
        "    time_series_length = study_config[\"global_parameters\"][\"time_series_length\"]\n",
        "    study_end_date = study_config[\"global_parameters\"][\"study_end_date\"]\n",
        "\n",
        "    # Step 1.4: Apply the cleansing and truncation protocol\n",
        "    df_clean_prices = _cleanse_and_truncate_data(\n",
        "        df_prices=df_prices,\n",
        "        time_series_length=time_series_length,\n",
        "        study_end_date=study_end_date\n",
        "    )\n",
        "\n",
        "    # Final verification of the output shape\n",
        "    if df_clean_prices.shape[0] != time_series_length:\n",
        "        raise RuntimeError(\n",
        "            \"Internal error: Final cleansed DataFrame shape is incorrect. \"\n",
        "            f\"Expected {time_series_length} rows, got {df_clean_prices.shape[0]}.\"\n",
        "        )\n",
        "\n",
        "    return df_clean_prices\n"
      ],
      "metadata": {
        "id": "LjcYJhrVlkOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preprocessing and Transformation\n",
        "\n",
        "def _calculate_log_returns(\n",
        "    df_prices: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the logarithmic returns of financial time series.\n",
        "\n",
        "    This function transforms a DataFrame of prices into a DataFrame of\n",
        "    logarithmic returns, which is a standard procedure in financial econometrics\n",
        "    to achieve stationarity and analyze percentage changes.\n",
        "\n",
        "    The transformation is based on the formula:\n",
        "    Equation (15): Logarithmic Return = ln(P_t) - ln(P_{t-1})\n",
        "\n",
        "    Args:\n",
        "        df_prices (pd.DataFrame): A DataFrame with a DatetimeIndex and columns\n",
        "                                  of asset prices. All prices must be positive.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame containing the log-returns for each asset.\n",
        "                      The resulting DataFrame will have one less row than the input.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input DataFrame contains non-positive prices.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure all price data is strictly positive before taking the logarithm.\n",
        "    if (df_prices <= 0).any().any():\n",
        "        raise ValueError(\"Input DataFrame contains non-positive prices. Logarithm is undefined.\")\n",
        "\n",
        "    # --- Log-Return Calculation ---\n",
        "    # Equation (15): Logarithmic Return = ln(P_t) - ln(P_{t-1})\n",
        "    # This is efficiently implemented by taking the log, then the difference.\n",
        "    df_log_returns = np.log(df_prices).diff().dropna()\n",
        "\n",
        "    # --- Output Verification ---\n",
        "    # Ensure the output is of the correct shape (N-1 rows).\n",
        "    expected_rows = df_prices.shape[0] - 1\n",
        "    if df_log_returns.shape[0] != expected_rows:\n",
        "        raise RuntimeError(\"Internal error: Log-return calculation resulted in unexpected shape.\")\n",
        "\n",
        "    return df_log_returns.astype(np.float64)\n",
        "\n",
        "\n",
        "def _characterize_log_returns(\n",
        "    df_log_returns: pd.DataFrame\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calculates key statistical properties for each log-return series.\n",
        "\n",
        "    This function computes the mean, standard deviation, skewness, and excess\n",
        "    kurtosis for each column in the log-returns DataFrame. These statistics\n",
        "    provide a quantitative description of the return distributions, as\n",
        "    presented in Table 1 of the source paper.\n",
        "\n",
        "    Formulas used for characterization:\n",
        "    - Skewness (γ₁): Based on Equation (16)\n",
        "    - Excess Kurtosis (γ₂): Based on Equation (17)\n",
        "\n",
        "    Args:\n",
        "        df_log_returns (pd.DataFrame): DataFrame of logarithmic returns.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, float]]: A nested dictionary where outer keys are\n",
        "                                     asset names (columns) and inner keys are\n",
        "                                     the names of the statistics ('mean',\n",
        "                                     'std_dev', 'skewness', 'ex_kurtosis').\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_log_returns, pd.DataFrame) or df_log_returns.empty:\n",
        "        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n",
        "\n",
        "    # Initialize the dictionary to store results\n",
        "    statistical_properties = {}\n",
        "\n",
        "    # Iterate over each asset (column) in the DataFrame\n",
        "    for asset_name in df_log_returns.columns:\n",
        "        # Extract the series for the current asset as a NumPy array for efficiency\n",
        "        series = df_log_returns[asset_name].values\n",
        "\n",
        "        # Calculate the mean of the series\n",
        "        mean_val = np.mean(series)\n",
        "\n",
        "        # Calculate the sample standard deviation (with Bessel's correction, ddof=1)\n",
        "        std_dev_val = np.std(series, ddof=1)\n",
        "\n",
        "        # Calculate the sample skewness using scipy.stats for robustness.\n",
        "        # bias=False provides an unbiased estimator.\n",
        "        # Equation (16): Skewness = μ₃ / σ³\n",
        "        skewness_val = stats.skew(series, bias=False)\n",
        "\n",
        "        # Calculate the sample excess kurtosis (Fisher's definition) using scipy.stats.\n",
        "        # This function already calculates (μ₄/σ⁴ - 3).\n",
        "        # bias=False provides an unbiased estimator.\n",
        "        # Equation (17): ex-Kurtosis = (μ₄ / σ⁴) - 3\n",
        "        ex_kurtosis_val = stats.kurtosis(series, fisher=True, bias=False)\n",
        "\n",
        "        # Store the computed statistics in the results dictionary\n",
        "        statistical_properties[asset_name] = {\n",
        "            \"mean\": mean_val,\n",
        "            \"std_dev\": std_dev_val,\n",
        "            \"skewness\": skewness_val,\n",
        "            \"ex_kurtosis\": ex_kurtosis_val,\n",
        "        }\n",
        "\n",
        "    return statistical_properties\n",
        "\n",
        "\n",
        "def _generate_shuffled_control_set(\n",
        "    df_log_returns: pd.DataFrame,\n",
        "    seed: int = 42\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a shuffled version of the log-return data for control analysis.\n",
        "\n",
        "    This function creates a surrogate dataset by randomly permuting the temporal\n",
        "    order of each asset's log-return series independently. This process destroys\n",
        "    all temporal correlations while preserving the exact marginal distribution\n",
        "    (i.e., mean, variance, and all higher-order moments) of each series.\n",
        "    This shuffled set serves as a null-hypothesis benchmark in subsequent\n",
        "    entropy and multifractal analyses.\n",
        "\n",
        "    Args:\n",
        "        df_log_returns (pd.DataFrame): The original DataFrame of log-returns.\n",
        "        seed (int, optional): A seed for the random number generator to ensure\n",
        "                              reproducibility. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with the same shape, index, and columns,\n",
        "                      but with the values in each column randomly shuffled.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_log_returns, pd.DataFrame) or df_log_returns.empty:\n",
        "        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n",
        "\n",
        "    # Create a deep copy to ensure the original DataFrame is not modified\n",
        "    df_shuffled = df_log_returns.copy()\n",
        "\n",
        "    # Initialize a modern random number generator for reproducibility and best practices\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Iterate over each asset (column) to shuffle it independently\n",
        "    for col in df_shuffled.columns:\n",
        "        # Permute the values of the column in-place using the seeded generator\n",
        "        df_shuffled[col] = rng.permutation(df_shuffled[col].values)\n",
        "\n",
        "    # --- Output Verification ---\n",
        "    # A sanity check to confirm the shuffling preserved the distribution (mean).\n",
        "    # Using a tolerance for floating point comparisons.\n",
        "    if not np.allclose(df_shuffled.mean(), df_log_returns.mean()):\n",
        "        raise RuntimeError(\"Internal error: Shuffling altered series mean unexpectedly.\")\n",
        "\n",
        "    return df_shuffled\n",
        "\n",
        "\n",
        "def preprocess_and_characterize_data(\n",
        "    df_clean_prices: pd.DataFrame,\n",
        "    seed: int = 42\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full data preprocessing and characterization pipeline (Task 2).\n",
        "\n",
        "    This function takes a clean price DataFrame and performs three key steps:\n",
        "    1. Calculates the logarithmic returns for each asset.\n",
        "    2. Computes a detailed statistical characterization of the log-return series.\n",
        "    3. Generates a shuffled surrogate dataset for control analysis.\n",
        "\n",
        "    Args:\n",
        "        df_clean_prices (pd.DataFrame): A clean, validated DataFrame of daily\n",
        "                                        closing prices from Task 1.\n",
        "        seed (int, optional): A random seed for generating the shuffled dataset,\n",
        "                              ensuring reproducibility. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Dict[str, float]]]: A tuple\n",
        "        containing:\n",
        "            - df_log_returns: The DataFrame of logarithmic returns.\n",
        "            - df_log_returns_shuffled: The shuffled control DataFrame.\n",
        "            - statistical_properties: A nested dictionary of statistical moments.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Log-Return Transformation\n",
        "    df_log_returns = _calculate_log_returns(df_clean_prices)\n",
        "\n",
        "    # Step 2.2: Statistical Characterization\n",
        "    statistical_properties = _characterize_log_returns(df_log_returns)\n",
        "\n",
        "    # Step 2.3: Shuffled Control Dataset Generation\n",
        "    df_log_returns_shuffled = _generate_shuffled_control_set(df_log_returns, seed)\n",
        "\n",
        "    return df_log_returns, df_log_returns_shuffled, statistical_properties\n"
      ],
      "metadata": {
        "id": "ra7DECNLmE5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: RCMSE Implementation Framework\n",
        "\n",
        "def _compute_sampen_components(\n",
        "    series: np.ndarray,\n",
        "    m: int,\n",
        "    r: float\n",
        ") -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Core computational engine for Sample Entropy pattern counting.\n",
        "\n",
        "    This highly optimized function calculates the number of matching template\n",
        "    vector pairs for embedding dimensions 'm' (n_m) and 'm+1' (n_m_plus_1).\n",
        "    It uses numpy.lib.stride_tricks to create zero-copy, memory-efficient\n",
        "    views for the embedding vectors, and then uses NumPy broadcasting for\n",
        "    ultra-fast, vectorized distance calculations.\n",
        "\n",
        "    Args:\n",
        "        series (np.ndarray): The 1D time series data, must be contiguous.\n",
        "        m (int): The embedding dimension (length of the template vectors).\n",
        "        r (float): The tolerance radius for matching.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[int, int]: A tuple containing:\n",
        "            - n_m: The total number of matching pairs of length 'm'.\n",
        "            - n_m_plus_1: The total number of matching pairs of length 'm+1'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Preparation ---\n",
        "    # Ensure the series is a 1D numpy array.\n",
        "    if not isinstance(series, np.ndarray) or series.ndim != 1:\n",
        "        raise TypeError(\"Input 'series' must be a 1D NumPy array.\")\n",
        "    # Get the length of the time series.\n",
        "    n = series.shape[0]\n",
        "    # Ensure the series is long enough for the given embedding dimension.\n",
        "    if n <= m:\n",
        "        # If the series is too short, no components can be formed.\n",
        "        return 0, 0\n",
        "\n",
        "    # --- Step 1: Embedding Vector Construction via Stride Tricks ---\n",
        "    # This is a highly efficient method to create sliding window views without copying data.\n",
        "    # Get the byte size of an element in the array.\n",
        "    itemsize = series.itemsize\n",
        "\n",
        "    # Create the embedding matrix for dimension m.\n",
        "    # Equation (1): Xm(i) = {xi, xi+1, ..., xi+m-1}\n",
        "    shape_m = (n - m + 1, m)\n",
        "    strides_m = (itemsize, itemsize)\n",
        "    x_m = np.lib.stride_tricks.as_strided(series, shape=shape_m, strides=strides_m)\n",
        "\n",
        "    # Create the embedding matrix for dimension m+1.\n",
        "    shape_m_plus_1 = (n - m, m + 1)\n",
        "    strides_m_plus_1 = (itemsize, itemsize)\n",
        "    x_m_plus_1 = np.lib.stride_tricks.as_strided(series, shape=shape_m_plus_1, strides=strides_m_plus_1)\n",
        "\n",
        "    # --- Step 2: Distance Calculation and Pattern Counting ---\n",
        "    # Calculate n_m: Number of matches for dimension m.\n",
        "    # This computes the Chebyshev distance (max absolute difference) between all pairs of vectors.\n",
        "    # Equation (2): d[Xm(i), Xm(j)] = max(|xi+k - xj+k|)\n",
        "    # The operation is fully vectorized using NumPy broadcasting for maximum speed.\n",
        "    dist_m = np.max(np.abs(x_m[:, np.newaxis, :] - x_m[np.newaxis, :, :]), axis=2)\n",
        "\n",
        "    # Count pairs where the distance is less than or equal to the tolerance r.\n",
        "    # We sum the boolean mask, subtract the diagonal (to exclude self-matches, i.e., i != j),\n",
        "    # and divide by 2 to correct for double-counting symmetric pairs (i,j) and (j,i).\n",
        "    n_m = (np.sum(dist_m <= r) - shape_m[0]) // 2\n",
        "\n",
        "    # Calculate n_m_plus_1: Number of matches for dimension m+1.\n",
        "    # The same vectorized logic is applied to the (m+1)-dimensional vectors.\n",
        "    dist_m_plus_1 = np.max(np.abs(x_m_plus_1[:, np.newaxis, :] - x_m_plus_1[np.newaxis, :, :]), axis=2)\n",
        "\n",
        "    # Count the unique matching pairs for dimension m+1.\n",
        "    n_m_plus_1 = (np.sum(dist_m_plus_1 <= r) - shape_m_plus_1[0]) // 2\n",
        "\n",
        "    return n_m, n_m_plus_1\n",
        "\n",
        "\n",
        "def compute_rcmse_profile(\n",
        "    series: np.ndarray,\n",
        "    rcmse_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes the Refined Composite Multiscale Sample Entropy (RCMSE) profile.\n",
        "\n",
        "    This function implements the full RCMSE algorithm as described in the paper.\n",
        "    It analyzes the irregularity of a time series across multiple time scales,\n",
        "    from 1 to 'max_scale_tau'. It uses an overlapping coarse-graining procedure\n",
        "    for improved stability, especially for shorter time series.\n",
        "\n",
        "    Methodology:\n",
        "    1. For scale τ=1, it calculates standard Sample Entropy.\n",
        "    2. For scales τ>1, it generates τ overlapping coarse-grained series.\n",
        "    3. It computes pattern counts (n_m, n_m+1) for each coarse-grained series.\n",
        "    4. It aggregates these counts across all τ series for that scale.\n",
        "    5. It calculates the final RCMSE value using the aggregated counts, as per\n",
        "       Equation (6): RCMSE = -ln( (Σ n^(m+1)) / (Σ n^m) ).\n",
        "\n",
        "    Args:\n",
        "        series (np.ndarray): The 1D log-return time series for a single asset.\n",
        "        rcmse_config (Dict[str, Any]): A dictionary containing the RCMSE\n",
        "                                       hyperparameters: 'embedding_dimension_m',\n",
        "                                       'tolerance_r_factor', 'max_scale_tau'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the results:\n",
        "            - 'rcmse_profile': A 1D NumPy array of RCMSE values for each scale.\n",
        "            - 'total_complexity': The sum of all values in the rcmse_profile.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(series, np.ndarray) or series.ndim != 1:\n",
        "        raise TypeError(\"Input 'series' must be a 1D NumPy array.\")\n",
        "    if not isinstance(rcmse_config, dict):\n",
        "        raise TypeError(\"Input 'rcmse_config' must be a dictionary.\")\n",
        "\n",
        "    # --- Hyperparameter Extraction ---\n",
        "    m = rcmse_config['embedding_dimension_m']\n",
        "    r_factor = rcmse_config['tolerance_r_factor']\n",
        "    max_tau = rcmse_config['max_scale_tau']\n",
        "\n",
        "    # --- Pre-computation ---\n",
        "    # Calculate the tolerance 'r' based on the standard deviation of the original series.\n",
        "    # This is a crucial step: 'r' is fixed across all scales.\n",
        "    r = r_factor * np.std(series, ddof=1)\n",
        "\n",
        "    # Get the length of the original series\n",
        "    n = series.shape[0]\n",
        "\n",
        "    # Initialize an array to store the RCMSE values for each scale\n",
        "    rcmse_values = np.zeros(max_tau)\n",
        "\n",
        "    # --- RCMSE Calculation across Scales ---\n",
        "    for tau in range(1, max_tau + 1):\n",
        "        # Initialize total pattern counts for the current scale 'tau'\n",
        "        total_n_m = 0\n",
        "        total_n_m_plus_1 = 0\n",
        "\n",
        "        # --- Case 1: Standard Sample Entropy (Scale tau = 1) ---\n",
        "        if tau == 1:\n",
        "            # Directly compute components on the original series\n",
        "            n_m, n_m_plus_1 = _compute_sampen_components(series, m, r)\n",
        "            total_n_m += n_m\n",
        "            total_n_m_plus_1 += n_m_plus_1\n",
        "\n",
        "        # --- Case 2: Refined Composite MSE (Scale tau > 1) ---\n",
        "        else:\n",
        "            # Generate 'tau' overlapping coarse-grained series\n",
        "            # Equation (5): y^(τ)_(k,j) = (1/τ) * Σ x_i\n",
        "            for k in range(tau):\n",
        "                # Create the k-th coarse-grained series\n",
        "                # We select the appropriate slice and then reshape and average\n",
        "                end_index = n - (n - k) % tau\n",
        "                coarse_grained_series = np.mean(series[k:end_index].reshape(-1, tau), axis=1)\n",
        "\n",
        "                # Ensure the series is long enough for at least one m+1 pattern\n",
        "                if coarse_grained_series.shape[0] > m:\n",
        "                    # Compute components for this coarse-grained series\n",
        "                    n_m, n_m_plus_1 = _compute_sampen_components(coarse_grained_series, m, r)\n",
        "                    # Aggregate the counts\n",
        "                    total_n_m += n_m\n",
        "                    total_n_m_plus_1 += n_m_plus_1\n",
        "\n",
        "        # --- Calculate Final RCMSE for the current scale 'tau' ---\n",
        "        # Equation (6): RCMSE(τ) = -ln( Σ(n_m+1) / Σ(n_m) )\n",
        "        # Check for undefined entropy (zero matches)\n",
        "        if total_n_m == 0 or total_n_m_plus_1 == 0:\n",
        "            # If no matches are found, entropy is undefined. Store as NaN.\n",
        "            rcmse_values[tau - 1] = np.nan\n",
        "        else:\n",
        "            # Calculate the RCMSE value and store it\n",
        "            rcmse_values[tau - 1] = -np.log(total_n_m_plus_1 / total_n_m)\n",
        "\n",
        "    # --- Calculate Total Complexity ---\n",
        "    # The total complexity is the sum of the entropy values over all scales.\n",
        "    # We use np.nansum to handle any potential NaN values gracefully.\n",
        "    total_complexity = np.nansum(rcmse_values)\n",
        "\n",
        "    # --- Package and Return Results ---\n",
        "    results = {\n",
        "        \"rcmse_profile\": rcmse_values,\n",
        "        \"total_complexity\": total_complexity\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "IzZqtMtrm0KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: MF-DFA Implementation Framework\n",
        "\n",
        "def _construct_mdfa_profile(\n",
        "    series: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the profile series for MF-DFA analysis.\n",
        "\n",
        "    This function implements Step 1 of the MF-DFA algorithm. It transforms the\n",
        "    original time series (typically stationary log-returns) into a non-stationary,\n",
        "    random walk-like profile by computing the cumulative sum of deviations from\n",
        "    the mean.\n",
        "\n",
        "    The transformation is based on the formula:\n",
        "    Equation (7): Y(i) = Σ_{k=1 to i} (x_k - <x>)\n",
        "\n",
        "    Args:\n",
        "        series (np.ndarray): A 1D NumPy array of the time series data.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The resulting profile series, Y(i).\n",
        "    \"\"\"\n",
        "    # Calculate the mean of the series\n",
        "    series_mean = np.mean(series)\n",
        "\n",
        "    # Center the series by subtracting the mean\n",
        "    centered_series = series - series_mean\n",
        "\n",
        "    # Compute the cumulative sum to create the profile\n",
        "    profile = np.cumsum(centered_series)\n",
        "\n",
        "    return profile\n",
        "\n",
        "def _compute_fluctuation_function(\n",
        "    profile: np.ndarray,\n",
        "    s_range: np.ndarray,\n",
        "    q_range: np.ndarray,\n",
        "    polynomial_order: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the q-order fluctuation function F_q(s) for all scales and moments.\n",
        "\n",
        "    This is the computational core of the MF-DFA algorithm, implementing Steps 2, 3,\n",
        "    and 4. For each scale 's' and moment 'q', it performs:\n",
        "    1. Bidirectional segmentation of the profile.\n",
        "    2. Polynomial detrending of each segment.\n",
        "    3. Calculation of the variance of the residuals.\n",
        "    4. Averaging of variances to compute F_q(s) as per Equation (10).\n",
        "\n",
        "    Args:\n",
        "        profile (np.ndarray): The profile series generated by _construct_mdfa_profile.\n",
        "        s_range (np.ndarray): An array of integer scales 's' to analyze.\n",
        "        q_range (np.ndarray): An array of moments 'q' to use.\n",
        "        polynomial_order (int): The degree of the polynomial for detrending.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D array of shape (len(q_range), len(s_range)) containing\n",
        "                    the F_q(s) values.\n",
        "    \"\"\"\n",
        "    # Get the length of the profile series\n",
        "    n = profile.shape[0]\n",
        "\n",
        "    # Initialize the matrix to store F_q(s) values\n",
        "    f_q_s = np.zeros((len(q_range), len(s_range)))\n",
        "\n",
        "    # Iterate over each scale in the s_range\n",
        "    for i, s in enumerate(s_range):\n",
        "        # --- Step 2: Segmentation ---\n",
        "        # Calculate the number of segments\n",
        "        n_s = n // s\n",
        "\n",
        "        # Reshape the profile for forward and backward segmentation\n",
        "        # This creates 2*n_s segments in total\n",
        "        forward_segments = profile[:n_s * s].reshape(n_s, s)\n",
        "        backward_segments = profile[n - n_s * s:].reshape(n_s, s)\n",
        "        all_segments = np.vstack([forward_segments, backward_segments])\n",
        "\n",
        "        # --- Step 3: Detrending ---\n",
        "        # Create the x-axis (time indices) for polynomial fitting\n",
        "        x_indices = np.arange(s)\n",
        "\n",
        "        # Fit polynomial and calculate variance for all segments in a vectorized way\n",
        "        # np.polyfit can operate on all segments at once if y is 2D\n",
        "        coeffs = np.polyfit(x_indices, all_segments.T, polynomial_order)\n",
        "        fitted_trends = np.polyval(coeffs, x_indices).T\n",
        "\n",
        "        # Calculate the variance (F^2(s,v)) for each segment\n",
        "        # Equations (8) & (9)\n",
        "        variances = np.mean((all_segments - fitted_trends)**2, axis=1)\n",
        "\n",
        "        # --- Step 4: q-order Fluctuation Function ---\n",
        "        # Iterate over each moment in the q_range\n",
        "        for j, q in enumerate(q_range):\n",
        "            # Use only segments with non-zero variance to avoid errors\n",
        "            valid_variances = variances[variances > 0]\n",
        "\n",
        "            # Handle the special case for q = 0\n",
        "            if q == 0:\n",
        "                # Equation (10) for q=0: Geometric mean\n",
        "                f_q_s[j, i] = np.exp(0.5 * np.mean(np.log(valid_variances)))\n",
        "            else:\n",
        "                # Equation (10) for q!=0: q-th order root mean square\n",
        "                f_q_s[j, i] = np.mean(valid_variances**(q / 2))**(1 / q)\n",
        "\n",
        "    return f_q_s\n",
        "\n",
        "def _extract_scaling_exponents(\n",
        "    f_q_s: np.ndarray,\n",
        "    s_range: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extracts the generalized Hurst exponents h(q) from F_q(s).\n",
        "\n",
        "    This function implements Step 5 of the MF-DFA algorithm. It performs a\n",
        "    log-log linear regression of F_q(s) against 's' for each moment 'q'.\n",
        "    The slope of this regression line is the generalized Hurst exponent h(q).\n",
        "\n",
        "    The relationship is based on the power law:\n",
        "    Equation (11): F_q(s) ~ s^h(q)\n",
        "\n",
        "    Args:\n",
        "        f_q_s (np.ndarray): The 2D matrix of fluctuation functions.\n",
        "        s_range (np.ndarray): The array of scales 's' used.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D array of the generalized Hurst exponents h(q).\n",
        "    \"\"\"\n",
        "    # Take the logarithm of the scales for the regression\n",
        "    log_s = np.log10(s_range)\n",
        "\n",
        "    # Initialize an array to store the h(q) values\n",
        "    h_q = np.zeros(f_q_s.shape[0])\n",
        "\n",
        "    # Iterate over each moment q\n",
        "    for i in range(f_q_s.shape[0]):\n",
        "        # Take the logarithm of the fluctuation function for the current q\n",
        "        log_f_q = np.log10(f_q_s[i, :])\n",
        "\n",
        "        # Perform linear regression (degree 1 polynomial fit)\n",
        "        # The slope of the fit is the h(q) exponent\n",
        "        coeffs = np.polyfit(log_s, log_f_q, 1)\n",
        "        h_q[i] = coeffs[0]\n",
        "\n",
        "    return h_q\n",
        "\n",
        "def _compute_singularity_spectrum(\n",
        "    h_q: np.ndarray,\n",
        "    q_range: np.ndarray\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculates the multifractal singularity spectrum from h(q).\n",
        "\n",
        "    This function performs the final step of the analysis, converting the\n",
        "    generalized Hurst exponents h(q) into the singularity spectrum f(α) via\n",
        "    a Legendre transform.\n",
        "\n",
        "    Formulas used:\n",
        "    - Equation (12): τ(q) = q*h(q) - 1\n",
        "    - Equation (13): α(q) = dτ(q)/dq\n",
        "    - Equation (13): f(α) = q*α - τ(q)\n",
        "\n",
        "    Args:\n",
        "        h_q (np.ndarray): The array of generalized Hurst exponents.\n",
        "        q_range (np.ndarray): The array of moments 'q'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the singularity spectrum:\n",
        "            - 'tau_q': The mass exponents.\n",
        "            - 'alpha': The Hölder exponents (singularity strengths).\n",
        "            - 'f_alpha': The singularity spectrum.\n",
        "            - 'delta_alpha': The width of the spectrum (a complexity measure).\n",
        "    \"\"\"\n",
        "    # Equation (12): Calculate the mass exponent tau(q)\n",
        "    tau_q = q_range * h_q - 1\n",
        "\n",
        "    # Equation (13): Calculate alpha(q) by differentiating tau(q) w.r.t. q\n",
        "    # np.gradient provides a robust numerical derivative\n",
        "    alpha = np.gradient(tau_q, q_range)\n",
        "\n",
        "    # Equation (13): Calculate the singularity spectrum f(alpha)\n",
        "    f_alpha = q_range * alpha - tau_q\n",
        "\n",
        "    # Calculate the width of the spectrum, a key measure of multifractality\n",
        "    delta_alpha = np.max(alpha) - np.min(alpha)\n",
        "\n",
        "    # Package the results\n",
        "    spectrum = {\n",
        "        \"tau_q\": tau_q,\n",
        "        \"alpha\": alpha,\n",
        "        \"f_alpha\": f_alpha,\n",
        "        \"delta_alpha\": delta_alpha\n",
        "    }\n",
        "\n",
        "    return spectrum\n",
        "\n",
        "def compute_mfdfa_analysis(\n",
        "    series: np.ndarray,\n",
        "    mdfa_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full Multifractal Detrended Fluctuation Analysis (MF-DFA).\n",
        "\n",
        "    This function serves as the main pipeline for Task 4. It takes a time series\n",
        "    and a configuration dictionary, and executes the complete MF-DFA workflow\n",
        "    to characterize the series' multifractal properties.\n",
        "\n",
        "    Args:\n",
        "        series (np.ndarray): The 1D log-return time series for a single asset.\n",
        "        mdfa_config (Dict[str, Any]): A dictionary containing the MF-DFA\n",
        "                                      hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all MF-DFA results,\n",
        "                        including h(q), the full singularity spectrum, and the\n",
        "                        spectrum width.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(series, np.ndarray) or series.ndim != 1:\n",
        "        raise TypeError(\"Input 'series' must be a 1D NumPy array.\")\n",
        "    if not isinstance(mdfa_config, dict):\n",
        "        raise TypeError(\"Input 'mdfa_config' must be a dictionary.\")\n",
        "\n",
        "    # --- Hyperparameter and Parameter Extraction ---\n",
        "    hp = mdfa_config['hyperparameters']\n",
        "    q_range = hp['q_range']\n",
        "    s_params = hp['s_range_generation_params']\n",
        "    polynomial_order = hp['polynomial_order']\n",
        "    n = series.shape[0]\n",
        "\n",
        "    # Dynamically generate the scale range 's'\n",
        "    s_min = s_params['s_min']\n",
        "    s_max = n // s_params['s_max_divisor']\n",
        "    s_range = np.unique(np.geomspace(s_min, s_max, num=s_params['num_points'], dtype=int))\n",
        "\n",
        "    # --- Execute MF-DFA Pipeline ---\n",
        "    # Step 1: Construct the profile\n",
        "    profile = _construct_mdfa_profile(series)\n",
        "\n",
        "    # Steps 2-4: Compute the fluctuation function F_q(s)\n",
        "    f_q_s = _compute_fluctuation_function(profile, s_range, q_range, polynomial_order)\n",
        "\n",
        "    # Step 5: Extract the scaling exponents h(q)\n",
        "    h_q = _extract_scaling_exponents(f_q_s, s_range)\n",
        "\n",
        "    # Final Step: Compute the singularity spectrum\n",
        "    singularity_spectrum = _compute_singularity_spectrum(h_q, q_range)\n",
        "\n",
        "    # --- Package and Return All Results ---\n",
        "    # The Hurst exponent H is h(q) for q=2\n",
        "    hurst_exponent = h_q[np.where(q_range == 2.0)[0][0]] if 2.0 in q_range else np.nan\n",
        "\n",
        "    results = {\n",
        "        \"h_q\": h_q,\n",
        "        \"hurst_exponent\": hurst_exponent,\n",
        "        \"singularity_spectrum\": singularity_spectrum,\n",
        "        \"q_range\": q_range,\n",
        "        \"s_range\": s_range,\n",
        "        \"f_q_s\": f_q_s\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "6n4wtYAvnXBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Shuffling Control Analysis Framework\n",
        "\n",
        "def _generate_shuffled_datasets(\n",
        "    df_log_returns: pd.DataFrame,\n",
        "    n_shuffles: int = 100,\n",
        "    seed: int = 42\n",
        ") -> List[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generates an ensemble of surrogate datasets by shuffling time series.\n",
        "\n",
        "    This function creates a specified number of surrogate datasets by randomly\n",
        "    permuting each column of the input log-return DataFrame independently. This\n",
        "    destroys temporal correlations while preserving the marginal distribution of\n",
        "    each series, creating a null-model ensemble for statistical testing.\n",
        "\n",
        "    Args:\n",
        "        df_log_returns (pd.DataFrame): The original DataFrame of log-returns.\n",
        "        n_shuffles (int, optional): The number of shuffled surrogate datasets\n",
        "                                    to generate. Defaults to 100.\n",
        "        seed (int, optional): A seed for the random number generator to ensure\n",
        "                              reproducibility. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        List[pd.DataFrame]: A list of shuffled pandas DataFrames.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_log_returns, pd.DataFrame) or df_log_returns.empty:\n",
        "        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n",
        "    if not isinstance(n_shuffles, int) or n_shuffles <= 0:\n",
        "        raise ValueError(\"'n_shuffles' must be a positive integer.\")\n",
        "\n",
        "    # Initialize a modern random number generator for reproducibility\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Create the list of shuffled datasets\n",
        "    shuffled_datasets = []\n",
        "\n",
        "    for _ in range(n_shuffles):\n",
        "        # Create a deep copy to avoid modifying the original or other surrogates\n",
        "        df_shuffled = df_log_returns.copy()\n",
        "\n",
        "        # Iterate over each column to shuffle it independently\n",
        "        for col in df_shuffled.columns:\n",
        "            # Permute the values of the column using the seeded generator\n",
        "            df_shuffled[col] = rng.permutation(df_shuffled[col].values)\n",
        "\n",
        "        # Add the new surrogate to the list\n",
        "        shuffled_datasets.append(df_shuffled)\n",
        "\n",
        "    return shuffled_datasets\n",
        "\n",
        "def _worker_compute_metrics(\n",
        "    args: Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any]]\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    A helper function for parallel processing to compute metrics for one surrogate.\n",
        "\n",
        "    Args:\n",
        "        args (Tuple): A tuple containing the shuffled DataFrame, RCMSE config,\n",
        "                      and MF-DFA config.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, float]]: A dictionary of computed metrics for the\n",
        "                                     given surrogate DataFrame.\n",
        "    \"\"\"\n",
        "    # Unpack arguments\n",
        "    df_shuffled, rcmse_config, mdfa_config = args\n",
        "\n",
        "    # Initialize dictionary to store results for this worker\n",
        "    worker_results = {}\n",
        "\n",
        "    # Analyze each asset in the shuffled DataFrame\n",
        "    for asset in df_shuffled.columns:\n",
        "        # Extract the time series as a NumPy array\n",
        "        series = df_shuffled[asset].values\n",
        "\n",
        "        # Compute RCMSE and extract SampEn at scale 1\n",
        "        rcmse_results = compute_rcmse_profile(series, rcmse_config)\n",
        "        sampen_tau1 = rcmse_results['rcmse_profile'][0]\n",
        "\n",
        "        # Compute MF-DFA and extract the spectrum width\n",
        "        mdfa_results = compute_mfdfa_analysis(series, mdfa_config)\n",
        "        delta_alpha = mdfa_results['singularity_spectrum']['delta_alpha']\n",
        "\n",
        "        # Store the results for this asset\n",
        "        worker_results[asset] = {\n",
        "            'sampen_tau1': sampen_tau1,\n",
        "            'delta_alpha': delta_alpha\n",
        "        }\n",
        "\n",
        "    return worker_results\n",
        "\n",
        "def _compute_shuffled_metrics(\n",
        "    shuffled_datasets: List[pd.DataFrame],\n",
        "    study_config: Dict[str, Any],\n",
        "    num_workers: int = -1\n",
        ") -> Dict[str, Dict[str, List[float]]]:\n",
        "    \"\"\"\n",
        "    Computes complexity metrics for the ensemble of shuffled datasets in parallel.\n",
        "\n",
        "    This function takes the list of surrogate datasets and applies the RCMSE and\n",
        "    MF-DFA analyses to each one, distributing the workload across multiple CPU\n",
        "    cores for efficiency. It collects the key complexity metrics (Sample Entropy\n",
        "    at scale 1 and multifractal spectrum width) for each shuffle.\n",
        "\n",
        "    Args:\n",
        "        shuffled_datasets (List[pd.DataFrame]): The list of surrogate datasets.\n",
        "        study_config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        num_workers (int, optional): The number of parallel processes to use.\n",
        "                                     Defaults to os.cpu_count() - 1.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, List[float]]]: A nested dictionary containing the\n",
        "                                           distribution of metrics for each asset.\n",
        "                                           e.g., {'BTC_USD': {'delta_alpha': [0.3, 0.32, ...]}}\n",
        "    \"\"\"\n",
        "    # Determine the number of workers for the pool\n",
        "    if num_workers < 1:\n",
        "        num_workers = max(1, os.cpu_count() - 1)\n",
        "\n",
        "    # Extract necessary configs\n",
        "    rcmse_config = study_config['analytical_modules']['rcmse_analysis']\n",
        "    mdfa_config = study_config['analytical_modules']['mdfa_analysis']\n",
        "\n",
        "    # Prepare arguments for the parallel map function\n",
        "    tasks = [(df, rcmse_config, mdfa_config) for df in shuffled_datasets]\n",
        "\n",
        "    # Initialize the final results dictionary\n",
        "    assets = shuffled_datasets[0].columns\n",
        "    collated_results = {asset: {'sampen_tau1': [], 'delta_alpha': []} for asset in assets}\n",
        "\n",
        "    # Run the computations in parallel\n",
        "    with Pool(processes=num_workers) as pool:\n",
        "        # map() distributes tasks and collects results in order\n",
        "        results_list = pool.map(_worker_compute_metrics, tasks)\n",
        "\n",
        "    # Process and collate the results from the parallel workers\n",
        "    for single_shuffle_result in results_list:\n",
        "        for asset in assets:\n",
        "            collated_results[asset]['sampen_tau1'].append(single_shuffle_result[asset]['sampen_tau1'])\n",
        "            collated_results[asset]['delta_alpha'].append(single_shuffle_result[asset]['delta_alpha'])\n",
        "\n",
        "    return collated_results\n",
        "\n",
        "def conduct_shuffling_analysis(\n",
        "    df_log_returns: pd.DataFrame,\n",
        "    original_results: Dict[str, Any],\n",
        "    study_config: Dict[str, Any],\n",
        "    n_shuffles: int = 100,\n",
        "    seed: int = 42\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full shuffling control analysis framework (Task 5).\n",
        "\n",
        "    This function performs a complete surrogate data analysis to test the\n",
        "    statistical significance of the complexity metrics found in the original\n",
        "    data. It generates an ensemble of shuffled datasets, computes their\n",
        "    complexity metrics in parallel, and then compares these to the original\n",
        "    results to quantify the presence of meaningful temporal structure.\n",
        "\n",
        "    Args:\n",
        "        df_log_returns (pd.DataFrame): The original DataFrame of log-returns.\n",
        "        original_results (Dict[str, Any]): The dictionary of results from the\n",
        "                                           analysis of the original data.\n",
        "        study_config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        n_shuffles (int, optional): The number of surrogate datasets to create.\n",
        "                                    Defaults to 100.\n",
        "        seed (int, optional): A seed for the random number generator.\n",
        "                              Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing the comparison\n",
        "                        between original and shuffled metrics for each asset.\n",
        "    \"\"\"\n",
        "    # --- Step 5.1: Surrogate Data Generation ---\n",
        "    shuffled_datasets = _generate_shuffled_datasets(df_log_returns, n_shuffles, seed)\n",
        "\n",
        "    # --- Step 5.2: Parallel Metrics Computation ---\n",
        "    shuffled_metrics_dist = _compute_shuffled_metrics(shuffled_datasets, study_config)\n",
        "\n",
        "    # --- Step 5.3: Nonlinearity Quantification ---\n",
        "    final_analysis_results = {}\n",
        "    assets = df_log_returns.columns\n",
        "\n",
        "    for asset in assets:\n",
        "        # --- Process Delta Alpha (Multifractal) ---\n",
        "        orig_delta_alpha = original_results[asset]['mdfa']['singularity_spectrum']['delta_alpha']\n",
        "        shuffled_delta_alphas = np.array(shuffled_metrics_dist[asset]['delta_alpha'])\n",
        "\n",
        "        mean_shuffled_da = np.mean(shuffled_delta_alphas)\n",
        "        std_shuffled_da = np.std(shuffled_delta_alphas)\n",
        "\n",
        "        # Calculate the reduction in complexity\n",
        "        nl_multifractal = (orig_delta_alpha - mean_shuffled_da) / orig_delta_alpha if orig_delta_alpha != 0 else 0\n",
        "        # Calculate the z-score (how significant is the original value?)\n",
        "        z_score_da = (orig_delta_alpha - mean_shuffled_da) / std_shuffled_da if std_shuffled_da != 0 else np.inf\n",
        "\n",
        "        # --- Process Sample Entropy (Irregularity) ---\n",
        "        orig_sampen = original_results[asset]['rcmse']['rcmse_profile'][0]\n",
        "        shuffled_sampens = np.array(shuffled_metrics_dist[asset]['sampen_tau1'])\n",
        "\n",
        "        mean_shuffled_se = np.mean(shuffled_sampens)\n",
        "        std_shuffled_se = np.std(shuffled_sampens)\n",
        "\n",
        "        # Calculate the increase in entropy (loss of regularity)\n",
        "        nl_entropy = (mean_shuffled_se - orig_sampen) / orig_sampen if orig_sampen != 0 else 0\n",
        "        # Calculate the z-score\n",
        "        z_score_se = (orig_sampen - mean_shuffled_se) / std_shuffled_se if std_shuffled_se != 0 else -np.inf\n",
        "\n",
        "        # Package all results for this asset\n",
        "        final_analysis_results[asset] = {\n",
        "            'delta_alpha': {\n",
        "                'original': orig_delta_alpha,\n",
        "                'shuffled_mean': mean_shuffled_da,\n",
        "                'shuffled_std': std_shuffled_da,\n",
        "                'z_score': z_score_da,\n",
        "                'nonlinearity_index': nl_multifractal\n",
        "            },\n",
        "            'sampen_tau1': {\n",
        "                'original': orig_sampen,\n",
        "                'shuffled_mean': mean_shuffled_se,\n",
        "                'shuffled_std': std_shuffled_se,\n",
        "                'z_score': z_score_se,\n",
        "                'nonlinearity_index': nl_entropy\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return final_analysis_results\n"
      ],
      "metadata": {
        "id": "Q0vsQrOBn9Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Orchestrator Function Creation\n",
        "\n",
        "def _compute_shuffled_metrics(\n",
        "    shuffled_datasets: List[pd.DataFrame],\n",
        "    study_config: Dict[str, Any],\n",
        "    num_workers: int = -1\n",
        ") -> Dict[str, Dict[str, List[float]]]:\n",
        "    \"\"\"\n",
        "    Computes complexity metrics for the ensemble of shuffled datasets in parallel.\n",
        "    [Amended to clarify its role as the source of raw metric distributions.]\n",
        "\n",
        "    This function takes the list of surrogate datasets and applies the RCMSE and\n",
        "    MF-DFA analyses to each one, distributing the workload across multiple CPU\n",
        "    cores for efficiency. It collects the key complexity metrics (Sample Entropy\n",
        "    at scale 1 and multifractal spectrum width) for each shuffle.\n",
        "\n",
        "    Args:\n",
        "        shuffled_datasets (List[pd.DataFrame]): The list of surrogate datasets.\n",
        "        study_config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        num_workers (int, optional): The number of parallel processes to use.\n",
        "                                     Defaults to os.cpu_count() - 1.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, List[float]]]: A nested dictionary containing the\n",
        "                                           raw distribution of metrics for each asset.\n",
        "                                           e.g., {'BTC_USD': {'delta_alpha': [0.3, 0.32, ...]}}\n",
        "    \"\"\"\n",
        "    # Determine the number of workers for the pool\n",
        "    if num_workers < 1:\n",
        "        num_workers = max(1, os.cpu_count() - 1)\n",
        "\n",
        "    # Extract necessary configs\n",
        "    rcmse_config = study_config['analytical_modules']['rcmse_analysis']\n",
        "    mdfa_config = study_config['analytical_modules']['mdfa_analysis']\n",
        "\n",
        "    # Prepare arguments for the parallel map function\n",
        "    tasks = [(df, rcmse_config, mdfa_config) for df in shuffled_datasets]\n",
        "\n",
        "    # Initialize the final results dictionary\n",
        "    assets = shuffled_datasets[0].columns\n",
        "    collated_results = {asset: {'sampen_tau1': [], 'delta_alpha': []} for asset in assets}\n",
        "\n",
        "    # Run the computations in parallel\n",
        "    with Pool(processes=num_workers) as pool:\n",
        "        # map() distributes tasks and collects results in order\n",
        "        results_list = pool.map(_worker_compute_metrics, tasks)\n",
        "\n",
        "    # Process and collate the results from the parallel workers\n",
        "    for single_shuffle_result in results_list:\n",
        "        for asset in assets:\n",
        "            collated_results[asset]['sampen_tau1'].append(single_shuffle_result[asset]['sampen_tau1'])\n",
        "            collated_results[asset]['delta_alpha'].append(single_shuffle_result[asset]['delta_alpha'])\n",
        "\n",
        "    # Return the raw, collated distributions\n",
        "    return collated_results\n",
        "\n",
        "\n",
        "def conduct_shuffling_analysis(\n",
        "    df_log_returns: pd.DataFrame,\n",
        "    original_results: Dict[str, Any],\n",
        "    study_config: Dict[str, Any],\n",
        "    n_shuffles: int = 100,\n",
        "    seed: int = 42\n",
        ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full shuffling control analysis framework (Task 5).\n",
        "    [Amended to return both summary statistics and raw distributions.]\n",
        "\n",
        "    This function performs a complete surrogate data analysis. It generates an\n",
        "    ensemble of shuffled datasets, computes their complexity metrics in parallel,\n",
        "    and then calculates summary statistics (mean, std, z-score) to compare\n",
        "    against the original results.\n",
        "\n",
        "    Args:\n",
        "        df_log_returns (pd.DataFrame): The original DataFrame of log-returns.\n",
        "        original_results (Dict[str, Any]): The dictionary of results from the\n",
        "                                           analysis of the original data.\n",
        "        study_config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        n_shuffles (int, optional): The number of surrogate datasets to create.\n",
        "                                    Defaults to 100.\n",
        "        seed (int, optional): A seed for the random number generator.\n",
        "                              Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, Any], Dict[str, Any]]: A tuple containing:\n",
        "            - summary_results: A dictionary with aggregated comparison metrics.\n",
        "            - raw_results: A dictionary with the raw lists of shuffled metrics.\n",
        "    \"\"\"\n",
        "    # --- Step 5.1: Surrogate Data Generation ---\n",
        "    shuffled_datasets = _generate_shuffled_datasets(df_log_returns, n_shuffles, seed)\n",
        "\n",
        "    # --- Step 5.2: Parallel Metrics Computation ---\n",
        "    # This now returns the raw distributions of metrics from the shuffled ensemble\n",
        "    shuffled_metrics_raw = _compute_shuffled_metrics(shuffled_datasets, study_config)\n",
        "\n",
        "    # --- Step 5.3: Nonlinearity Quantification (Summary Statistics) ---\n",
        "    summary_results = {}\n",
        "    assets = df_log_returns.columns\n",
        "\n",
        "    for asset in assets:\n",
        "        # --- Process Delta Alpha (Multifractal) ---\n",
        "        orig_delta_alpha = original_results[asset]['mdfa']['singularity_spectrum']['delta_alpha']\n",
        "        shuffled_delta_alphas = np.array(shuffled_metrics_raw[asset]['delta_alpha'])\n",
        "\n",
        "        mean_shuffled_da = np.mean(shuffled_delta_alphas)\n",
        "        std_shuffled_da = np.std(shuffled_delta_alphas)\n",
        "\n",
        "        nl_multifractal = (orig_delta_alpha - mean_shuffled_da) / orig_delta_alpha if orig_delta_alpha != 0 else 0\n",
        "        z_score_da = (orig_delta_alpha - mean_shuffled_da) / std_shuffled_da if std_shuffled_da != 0 else np.inf\n",
        "\n",
        "        # --- Process Sample Entropy (Irregularity) ---\n",
        "        orig_sampen = original_results[asset]['rcmse']['rcmse_profile'][0]\n",
        "        shuffled_sampens = np.array(shuffled_metrics_raw[asset]['sampen_tau1'])\n",
        "\n",
        "        mean_shuffled_se = np.mean(shuffled_sampens)\n",
        "        std_shuffled_se = np.std(shuffled_sampens)\n",
        "\n",
        "        nl_entropy = (mean_shuffled_se - orig_sampen) / orig_sampen if orig_sampen != 0 else 0\n",
        "        z_score_se = (orig_sampen - mean_shuffled_se) / std_shuffled_se if std_shuffled_se != 0 else -np.inf\n",
        "\n",
        "        # Package the summary results for this asset\n",
        "        summary_results[asset] = {\n",
        "            'delta_alpha': {\n",
        "                'original': orig_delta_alpha,\n",
        "                'shuffled_mean': mean_shuffled_da,\n",
        "                'shuffled_std': std_shuffled_da,\n",
        "                'z_score': z_score_da,\n",
        "                'nonlinearity_index': nl_multifractal\n",
        "            },\n",
        "            'sampen_tau1': {\n",
        "                'original': orig_sampen,\n",
        "                'shuffled_mean': mean_shuffled_se,\n",
        "                'shuffled_std': std_shuffled_se,\n",
        "                'z_score': z_score_se,\n",
        "                'nonlinearity_index': nl_entropy\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Return both the summary and the raw distributions\n",
        "    return summary_results, shuffled_metrics_raw\n",
        "\n",
        "\n",
        "def run_complexity_analysis_pipeline(\n",
        "    df_prices: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete, end-to-end financial complexity analysis pipeline.\n",
        "\n",
        "    This master function serves as the single entry point to execute the entire\n",
        "    research methodology described in the paper.\n",
        "\n",
        "    Args:\n",
        "        df_prices (pd.DataFrame): The raw DataFrame of daily closing prices.\n",
        "        study_config (Dict[str, Any]): The configuration dictionary for the study.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all results.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Task 1: Input Validation and Data Quality Assurance ---\n",
        "        logging.info(\"Starting Task 1: Data Validation and Preparation...\")\n",
        "        df_clean_prices = validate_and_prepare_data(df_prices, study_config)\n",
        "        logging.info(\"Task 1 completed successfully.\")\n",
        "\n",
        "        # --- Task 2: Data Preprocessing and Transformation ---\n",
        "        logging.info(\"Starting Task 2: Data Preprocessing and Characterization...\")\n",
        "        df_log_returns, _, statistical_properties = preprocess_and_characterize_data(df_clean_prices)\n",
        "        logging.info(\"Task 2 completed successfully.\")\n",
        "\n",
        "        # Initialize the main results dictionary\n",
        "        all_results = {asset: {} for asset in df_log_returns.columns}\n",
        "\n",
        "        # --- Tasks 3 & 4: Per-Asset Complexity Analysis ---\n",
        "        for asset in df_log_returns.columns:\n",
        "            logging.info(f\"Analyzing asset: {asset}...\")\n",
        "            series = df_log_returns[asset].values\n",
        "            all_results[asset]['stats'] = statistical_properties[asset]\n",
        "\n",
        "            logging.info(f\"  Running RCMSE analysis for {asset}...\")\n",
        "            rcmse_config = study_config['analytical_modules']['rcmse_analysis']\n",
        "            all_results[asset]['rcmse'] = compute_rcmse_profile(series, rcmse_config)\n",
        "\n",
        "            logging.info(f\"  Running MF-DFA analysis for {asset}...\")\n",
        "            mdfa_config = study_config['analytical_modules']['mdfa_analysis']\n",
        "            all_results[asset]['mdfa'] = compute_mfdfa_analysis(series, mdfa_config)\n",
        "\n",
        "        # --- Task 5: Shuffling Control Analysis ---\n",
        "        logging.info(\"Starting Task 5: Shuffling Control Analysis...\")\n",
        "        # The call now unpacks two results: the summary and the raw distributions\n",
        "        shuffling_summary, shuffling_raw = conduct_shuffling_analysis(\n",
        "            df_log_returns, all_results, study_config\n",
        "        )\n",
        "\n",
        "        # Add both results to the main dictionary under distinct, clear keys\n",
        "        all_results['shuffling_analysis_summary'] = shuffling_summary\n",
        "        all_results['shuffling_analysis_raw'] = shuffling_raw\n",
        "        logging.info(\"Task 5 completed successfully.\")\n",
        "\n",
        "        logging.info(\"Complexity analysis pipeline finished successfully.\")\n",
        "        return all_results\n",
        "\n",
        "    except (ValueError, TypeError, KeyError, RuntimeError) as e:\n",
        "        logging.error(f\"Pipeline failed with an error: {e}\")\n",
        "        raise\n",
        "\n",
        "def validate_analysis_results(\n",
        "    analysis_results: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a quality control check on the final analysis results.\n",
        "\n",
        "    This function runs a series of \"sanity checks\" on the computed metrics to\n",
        "    ensure they are physically and theoretically plausible. It verifies the\n",
        "    properties of entropy, multifractal spectra, and the effects of shuffling.\n",
        "\n",
        "    Args:\n",
        "        analysis_results (Dict[str, Any]): The comprehensive results dictionary\n",
        "                                            from the main analysis pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A report dictionary summarizing the pass/fail status\n",
        "                        of each validation check.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting final results validation...\")\n",
        "    validation_report = {'summary': {'overall_status': 'PASS'}, 'details': {}}\n",
        "    error_list = []\n",
        "\n",
        "    assets = [k for k in analysis_results.keys() if k != 'shuffling_analysis']\n",
        "\n",
        "    for asset in assets:\n",
        "        asset_report = {}\n",
        "        # --- RCMSE Validation ---\n",
        "        rcmse_profile = analysis_results[asset]['rcmse']['rcmse_profile']\n",
        "        # Check if all entropy values are non-negative (ignoring NaNs)\n",
        "        if not np.all(rcmse_profile[~np.isnan(rcmse_profile)] >= 0):\n",
        "            msg = f\"{asset}: RCMSE profile contains negative values.\"\n",
        "            error_list.append(msg)\n",
        "            asset_report['rcmse_non_negative'] = f'FAIL - {msg}'\n",
        "        else:\n",
        "            asset_report['rcmse_non_negative'] = 'PASS'\n",
        "\n",
        "        # --- MF-DFA Validation ---\n",
        "        mdfa = analysis_results[asset]['mdfa']\n",
        "        spec = mdfa['singularity_spectrum']\n",
        "        # Check if spectrum width is non-negative\n",
        "        if not spec['delta_alpha'] >= 0:\n",
        "            msg = f\"{asset}: MF-DFA delta_alpha is negative ({spec['delta_alpha']:.4f}).\"\n",
        "            error_list.append(msg)\n",
        "            asset_report['mdfa_delta_alpha_positive'] = f'FAIL - {msg}'\n",
        "        else:\n",
        "            asset_report['mdfa_delta_alpha_positive'] = 'PASS'\n",
        "\n",
        "        # Check if f(alpha) peak is close to 1.0\n",
        "        f_alpha_max = np.max(spec['f_alpha'])\n",
        "        if not np.isclose(f_alpha_max, 1.0, atol=0.15):\n",
        "            msg = f\"{asset}: MF-DFA f(alpha) peak is not close to 1.0 (is {f_alpha_max:.4f}).\"\n",
        "            error_list.append(msg)\n",
        "            asset_report['mdfa_f_alpha_peak'] = f'FAIL - {msg}'\n",
        "        else:\n",
        "            asset_report['mdfa_f_alpha_peak'] = 'PASS'\n",
        "\n",
        "        # Check if Hurst exponent is in a plausible range\n",
        "        H = mdfa['hurst_exponent']\n",
        "        if not (0.3 < H < 0.8):\n",
        "            msg = f\"{asset}: Hurst exponent H is outside plausible range [0.3, 0.8] (is {H:.4f}).\"\n",
        "            error_list.append(msg)\n",
        "            asset_report['mdfa_hurst_plausible'] = f'FAIL - {msg}'\n",
        "        else:\n",
        "            asset_report['mdfa_hurst_plausible'] = 'PASS'\n",
        "\n",
        "        # --- Shuffling Analysis Validation ---\n",
        "        shuffling = analysis_results['shuffling_analysis'][asset]\n",
        "        # Check if shuffling reduced multifractality (positive z-score)\n",
        "        if not shuffling['delta_alpha']['z_score'] > 2.0:\n",
        "            msg = f\"{asset}: Shuffling did not significantly reduce delta_alpha (z-score: {shuffling['delta_alpha']['z_score']:.2f}).\"\n",
        "            error_list.append(msg)\n",
        "            asset_report['shuffling_delta_alpha_reduction'] = f'FAIL - {msg}'\n",
        "        else:\n",
        "            asset_report['shuffling_delta_alpha_reduction'] = 'PASS'\n",
        "\n",
        "        # Check if shuffling increased entropy (negative z-score)\n",
        "        if not shuffling['sampen_tau1']['z_score'] < -2.0:\n",
        "            msg = f\"{asset}: Shuffling did not significantly increase SampEn (z-score: {shuffling['sampen_tau1']['z_score']:.2f}).\"\n",
        "            error_list.append(msg)\n",
        "            asset_report['shuffling_sampen_increase'] = f'FAIL - {msg}'\n",
        "        else:\n",
        "            asset_report['shuffling_sampen_increase'] = 'PASS'\n",
        "\n",
        "        validation_report['details'][asset] = asset_report\n",
        "\n",
        "    if error_list:\n",
        "        validation_report['summary']['overall_status'] = 'FAIL'\n",
        "        validation_report['summary']['errors'] = error_list\n",
        "        logging.warning(\"Results validation failed with issues.\")\n",
        "    else:\n",
        "        logging.info(\"Results validation passed successfully.\")\n",
        "\n",
        "    return validation_report\n"
      ],
      "metadata": {
        "id": "1MN7L-3Mo0s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Robustness Analysis Implementation\n",
        "\n",
        "def _sensitivity_worker(\n",
        "    args: Tuple[pd.DataFrame, Dict[str, Any], Tuple[str, ...], Tuple[Any, ...]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Worker function for parallel sensitivity analysis. Executes one pipeline run.\n",
        "\n",
        "    Args:\n",
        "        args: A tuple containing the raw price data, the base study config,\n",
        "              a tuple of parameter names being varied, and a tuple of their values.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with the parameters used and the key results (complexity ranks).\n",
        "    \"\"\"\n",
        "    # Unpack arguments\n",
        "    df_prices, base_config, param_names, param_values = args\n",
        "\n",
        "    # Create a deep copy of the config to modify for this run\n",
        "    run_config = copy.deepcopy(base_config)\n",
        "\n",
        "    # Update the config with the current parameter combination\n",
        "    param_dict = {}\n",
        "    for name, value in zip(param_names, param_values):\n",
        "        # Navigate the nested dictionary to set the parameter value\n",
        "        keys = name.split('.')\n",
        "        d = run_config\n",
        "        for key in keys[:-1]:\n",
        "            d = d[key]\n",
        "        d[keys[-1]] = value\n",
        "        param_dict[name] = value\n",
        "\n",
        "    try:\n",
        "        # Run the full analysis pipeline with the modified config\n",
        "        results = run_complexity_analysis_pipeline(df_prices, run_config)\n",
        "\n",
        "        # Extract key results for ranking: RCMSE total complexity and MF-DFA delta_alpha\n",
        "        assets = [k for k in results.keys() if k != 'shuffling_analysis']\n",
        "        rcmse_scores = {asset: results[asset]['rcmse']['total_complexity'] for asset in assets}\n",
        "        mdfa_scores = {asset: results[asset]['mdfa']['singularity_spectrum']['delta_alpha'] for asset in assets}\n",
        "\n",
        "        # Convert scores to ranks (higher score = higher rank)\n",
        "        rcmse_ranks = stats.rankdata(list(rcmse_scores.values()), method='ordinal')\n",
        "        mdfa_ranks = stats.rankdata(list(mdfa_scores.values()), method='ordinal')\n",
        "\n",
        "        # Combine results\n",
        "        output = param_dict\n",
        "        for i, asset in enumerate(assets):\n",
        "            output[f'rank_rcmse_{asset}'] = rcmse_ranks[i]\n",
        "            output[f'rank_mdfa_{asset}'] = mdfa_ranks[i]\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log the error and return a failure indicator\n",
        "        logging.error(f\"Sensitivity run failed for params {param_dict}: {e}\")\n",
        "        return {**param_dict, 'status': 'FAIL'}\n",
        "\n",
        "\n",
        "def run_parameter_sensitivity_analysis(\n",
        "    df_prices: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    parameter_grid: Dict[str, List[Any]],\n",
        "    num_workers: int = -1\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Conducts a systematic sensitivity analysis on key model hyperparameters.\n",
        "\n",
        "    This function explores how the final complexity rankings of assets change\n",
        "    when key hyperparameters (e.g., embedding dimension 'm', polynomial order)\n",
        "    are varied. It runs the entire analysis pipeline for each combination of\n",
        "    parameters in the provided grid, distributing the computations in parallel.\n",
        "\n",
        "    Args:\n",
        "        df_prices (pd.DataFrame): The raw DataFrame of daily closing prices.\n",
        "        base_config (Dict[str, Any]): The baseline study configuration.\n",
        "        parameter_grid (Dict[str, List[Any]]): A dictionary where keys are the\n",
        "            full path to a hyperparameter (e.g., 'analytical_modules.rcmse_analysis.\n",
        "            hyperparameters.embedding_dimension_m') and values are lists of\n",
        "            alternative values to test.\n",
        "        num_workers (int, optional): Number of parallel processes. Defaults to\n",
        "                                     os.cpu_count() - 1.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the complexity rankings for each\n",
        "                      parameter combination, allowing for stability assessment.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting parameter sensitivity analysis...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if not parameter_grid:\n",
        "        logging.warning(\"Parameter grid is empty. Skipping sensitivity analysis.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Determine the number of workers\n",
        "    if num_workers < 1:\n",
        "        num_workers = max(1, os.cpu_count() - 1)\n",
        "\n",
        "    # Generate all combinations of parameters from the grid\n",
        "    param_names = list(parameter_grid.keys())\n",
        "    param_values_list = list(parameter_grid.values())\n",
        "    param_combinations = list(itertools.product(*param_values_list))\n",
        "\n",
        "    logging.info(f\"Testing {len(param_combinations)} parameter combinations across {num_workers} workers.\")\n",
        "\n",
        "    # Prepare tasks for the multiprocessing pool\n",
        "    tasks = [(df_prices, base_config, tuple(param_names), combo) for combo in param_combinations]\n",
        "\n",
        "    # Run the analysis in parallel\n",
        "    with Pool(processes=num_workers) as pool:\n",
        "        results_list = pool.map(_sensitivity_worker, tasks)\n",
        "\n",
        "    # Filter out any failed runs and convert the results to a DataFrame\n",
        "    successful_results = [res for res in results_list if res.get('status') != 'FAIL']\n",
        "    results_df = pd.DataFrame(successful_results)\n",
        "\n",
        "    logging.info(\"Parameter sensitivity analysis completed.\")\n",
        "    return results_df\n",
        "\n",
        "def conduct_statistical_significance_tests(\n",
        "    shuffling_summary: Dict[str, Any],\n",
        "    shuffling_raw: Dict[str, Any],\n",
        "    alpha: float = 0.05\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs statistical tests on the shuffling analysis results.\n",
        "    [Amended to correctly access summary and raw shuffling data.]\n",
        "\n",
        "    This function assesses the statistical significance of the difference\n",
        "    between the original complexity metrics and the distribution of metrics\n",
        "    from the surrogate (shuffled) data. It calculates empirical p-values,\n",
        "    effect sizes (z-scores), and applies a Bonferroni correction to account\n",
        "    for multiple comparisons.\n",
        "\n",
        "    Args:\n",
        "        shuffling_summary (Dict[str, Any]): The dictionary containing the\n",
        "            aggregated comparison metrics (original value, shuffled mean, etc.).\n",
        "        shuffling_raw (Dict[str, Any]): The dictionary containing the raw lists\n",
        "            of metrics from each shuffle.\n",
        "        alpha (float, optional): The significance level for hypothesis testing.\n",
        "                                 Defaults to 0.05.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing detailed statistical test\n",
        "                        results for each asset and metric.\n",
        "    \"\"\"\n",
        "    logging.info(\"Conducting statistical significance tests...\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if not shuffling_summary or not shuffling_raw:\n",
        "        raise ValueError(\"Input dictionaries for shuffling analysis cannot be empty.\")\n",
        "\n",
        "    assets = list(shuffling_summary.keys())\n",
        "    metrics = list(shuffling_summary[assets[0]].keys())\n",
        "    num_tests = len(assets) * len(metrics)\n",
        "\n",
        "    # Apply Bonferroni correction to the significance level\n",
        "    corrected_alpha = alpha / num_tests\n",
        "\n",
        "    test_report = {'summary': {'alpha': alpha, 'corrected_alpha': corrected_alpha}, 'details': {}}\n",
        "\n",
        "    for asset in assets:\n",
        "        asset_report = {}\n",
        "        for metric in metrics:\n",
        "            # --- Data Extraction from Correct Sources ---\n",
        "            # Get the summary data for the current metric\n",
        "            metric_summary = shuffling_summary[asset][metric]\n",
        "            # Get the original value from the summary\n",
        "            original_value = metric_summary['original']\n",
        "            # Get the raw shuffled values for testing\n",
        "            shuffled_values = np.array(shuffling_raw[asset][metric])\n",
        "            n_shuffles = len(shuffled_values)\n",
        "\n",
        "            # --- Empirical P-value Calculation ---\n",
        "            # This is a one-sided test based on the expected direction of change.\n",
        "            if metric == 'delta_alpha':\n",
        "                # Test hypothesis: original > shuffled. p-value is the probability\n",
        "                # of observing a shuffled value as large or larger than the original.\n",
        "                p_value = np.sum(shuffled_values >= original_value) / n_shuffles\n",
        "            else:  # For sampen_tau1\n",
        "                # Test hypothesis: original < shuffled. p-value is the probability\n",
        "                # of observing a shuffled value as small or smaller than the original.\n",
        "                p_value = np.sum(shuffled_values <= original_value) / n_shuffles\n",
        "\n",
        "            # --- Bootstrap Confidence Interval for the Shuffled Mean ---\n",
        "            # Generate bootstrap samples of the mean\n",
        "            bootstrap_means = [np.mean(np.random.choice(shuffled_values, size=n_shuffles, replace=True)) for _ in range(1000)]\n",
        "            # Calculate the 95% confidence interval\n",
        "            ci_lower = np.percentile(bootstrap_means, 2.5)\n",
        "            ci_upper = np.percentile(bootstrap_means, 97.5)\n",
        "\n",
        "            # --- Final Assessment ---\n",
        "            # Determine if the result is statistically significant after correction\n",
        "            is_significant = p_value < corrected_alpha\n",
        "\n",
        "            # Package the detailed statistical report for this metric\n",
        "            asset_report[metric] = {\n",
        "                'p_value': p_value,\n",
        "                'is_significant_at_corrected_alpha': is_significant,\n",
        "                'z_score': metric_summary['z_score'],\n",
        "                'shuffled_mean_ci_95': (ci_lower, ci_upper)\n",
        "            }\n",
        "        # Add the asset's full report to the main report\n",
        "        test_report['details'][asset] = asset_report\n",
        "\n",
        "    logging.info(\"Statistical significance testing completed.\")\n",
        "    return test_report\n"
      ],
      "metadata": {
        "id": "Ct_1CsyFp1j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "\n",
        "def generate_summary_tables(\n",
        "    analysis_results: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generates publication-quality summary tables from the analysis results.\n",
        "\n",
        "    This function processes the raw results dictionary and formats the key\n",
        "    findings into pandas DataFrames that replicate the main tables presented\n",
        "    in the source research paper.\n",
        "\n",
        "    Args:\n",
        "        analysis_results (Dict[str, Any]): The comprehensive results dictionary\n",
        "                                            from the main analysis pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary of pandas DataFrames, where each\n",
        "                                 key corresponds to a table number (e.g., 'Table1').\n",
        "    \"\"\"\n",
        "    logging.info(\"Generating summary tables...\")\n",
        "    assets = [k for k in analysis_results.keys() if k not in ['shuffling_analysis_summary', 'shuffling_analysis_raw']]\n",
        "    tables = {}\n",
        "\n",
        "    # --- Table 1: Skewness, ex-Kurtosis, and standard deviation ---\n",
        "    table1_data = {asset: {\n",
        "        'Skewness': analysis_results[asset]['stats']['skewness'],\n",
        "        'ex-Kurtosis': analysis_results[asset]['stats']['ex_kurtosis'],\n",
        "        'Standard Deviation': analysis_results[asset]['stats']['std_dev']\n",
        "    } for asset in assets}\n",
        "    df_table1 = pd.DataFrame(table1_data).T\n",
        "    tables['Table1_Statistical_Properties'] = df_table1\n",
        "\n",
        "    # --- Table 2: RCMSE Complexity values ---\n",
        "    table2_data = {asset: {\n",
        "        'Complexity': analysis_results[asset]['rcmse']['total_complexity'],\n",
        "        'SampEn (τ=1)': analysis_results[asset]['rcmse']['rcmse_profile'][0]\n",
        "    } for asset in assets}\n",
        "    df_table2 = pd.DataFrame(table2_data).T\n",
        "    tables['Table2_RCMSE_Complexity'] = df_table2\n",
        "\n",
        "    # --- Table 3: MF-DFA Spectrum Widths ---\n",
        "    table3_data = {asset: {\n",
        "        'Spectrum Width': analysis_results[asset]['mdfa']['singularity_spectrum']['delta_alpha'],\n",
        "        'α_max': np.max(analysis_results[asset]['mdfa']['singularity_spectrum']['alpha']),\n",
        "        'α_min': np.min(analysis_results[asset]['mdfa']['singularity_spectrum']['alpha'])\n",
        "    } for asset in assets}\n",
        "    df_table3 = pd.DataFrame(table3_data).T\n",
        "    tables['Table3_MFDFA_Spectrum_Widths'] = df_table3\n",
        "\n",
        "    # --- Table 4: Hurst Exponents ---\n",
        "    table4_data = {asset: {\n",
        "        'Hurst Exponent': analysis_results[asset]['mdfa']['hurst_exponent']\n",
        "    } for asset in assets}\n",
        "    df_table4 = pd.DataFrame(table4_data).T\n",
        "    tables['Table4_Hurst_Exponents'] = df_table4\n",
        "\n",
        "    logging.info(\"Summary tables generated successfully.\")\n",
        "    return tables\n",
        "\n",
        "def generate_publication_visualizations(\n",
        "    analysis_results: Dict[str, Any],\n",
        "    output_dir: str = 'results/figures'\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Generates and saves publication-quality visualizations of the results.\n",
        "\n",
        "    Args:\n",
        "        analysis_results (Dict[str, Any]): The comprehensive results dictionary.\n",
        "        output_dir (str, optional): Directory to save the plot images.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary mapping figure names to their file paths.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Generating visualizations in '{output_dir}'...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    paths = {}\n",
        "    assets = [k for k in analysis_results.keys() if k not in ['shuffling_analysis_summary', 'shuffling_analysis_raw']]\n",
        "\n",
        "    # --- Figure 6: RCMSE Profile ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    for asset in assets:\n",
        "        ax.plot(analysis_results[asset]['rcmse']['rcmse_profile'], label=asset, marker='o', markersize=3, linestyle='-')\n",
        "    ax.set_xlabel('Scale (τ)')\n",
        "    ax.set_ylabel('RCMSE')\n",
        "    ax.set_title('Refined Composite Multiscale Sample Entropy (RCMSE) Profiles')\n",
        "    ax.legend()\n",
        "    path = os.path.join(output_dir, 'Fig6_RCMSE_Profiles.png')\n",
        "    fig.savefig(path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    paths['Fig6_RCMSE_Profiles'] = path\n",
        "\n",
        "    # --- Figure 8: Singularity Spectra ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    for asset in assets:\n",
        "        spec = analysis_results[asset]['mdfa']['singularity_spectrum']\n",
        "        ax.plot(spec['alpha'], spec['f_alpha'], label=f\"{asset} (Δα = {spec['delta_alpha']:.2f})\")\n",
        "    ax.set_xlabel('Hölder Exponent (α)')\n",
        "    ax.set_ylabel('Singularity Spectrum (f(α))')\n",
        "    ax.set_title('Multifractal Singularity Spectra')\n",
        "    ax.legend()\n",
        "    path = os.path.join(output_dir, 'Fig8_Singularity_Spectra.png')\n",
        "    fig.savefig(path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    paths['Fig8_Singularity_Spectra'] = path\n",
        "\n",
        "    logging.info(\"Visualizations generated successfully.\")\n",
        "    return paths\n",
        "\n",
        "\n",
        "def _get_nested_value(\n",
        "    data: Dict[str, Any],\n",
        "    path: Tuple[str, ...]\n",
        ") -> Tuple[Optional[Any], bool]:\n",
        "    \"\"\"\n",
        "    Safely retrieves a value from a nested dictionary using a path tuple.\n",
        "\n",
        "    Args:\n",
        "        data (Dict[str, Any]): The nested dictionary to search within.\n",
        "        path (Tuple[str, ...]): A tuple of keys representing the path to the value.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[Any], bool]: A tuple containing:\n",
        "            - The retrieved value, or None if the path is invalid.\n",
        "            - A boolean indicating if the retrieval was successful.\n",
        "    \"\"\"\n",
        "    # Start at the top level of the dictionary.\n",
        "    current_level = data\n",
        "    # Iterate through the keys in the path tuple to traverse the dictionary.\n",
        "    for key in path:\n",
        "        # Check if the current level is a dictionary and contains the next key.\n",
        "        if isinstance(current_level, dict) and key in current_level:\n",
        "            # Move to the next level down.\n",
        "            current_level = current_level[key]\n",
        "        else:\n",
        "            # If the key is not found or the level is not a dict, the path is invalid.\n",
        "            logging.warning(f\"Benchmark verification failed: Path '{path}' not found at key '{key}'.\")\n",
        "            return None, False\n",
        "    # If the loop completes, the value was found successfully.\n",
        "    return current_level, True\n",
        "\n",
        "def verify_results_against_benchmarks(\n",
        "    analysis_results: Dict[str, Any],\n",
        "    benchmarks: Dict[str, Dict[str, Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Verifies key results against established benchmarks from the source paper.\n",
        "    [Re-implemented with a robust and safe nested data access mechanism.]\n",
        "\n",
        "    This function provides a final, critical layer of validation by comparing\n",
        "    key quantitative outputs of the analysis pipeline against known, expected\n",
        "    values (e.g., from the original research paper). It uses a robust method\n",
        "    for accessing nested data and provides a detailed, transparent report of\n",
        "    the outcome of each check.\n",
        "\n",
        "    The benchmark dictionary must use a tuple of keys for the 'path'. For example:\n",
        "    'path': ('BTC-USD', 'rcmse', 'total_complexity')\n",
        "\n",
        "    Args:\n",
        "        analysis_results (Dict[str, Any]): The comprehensive results dictionary\n",
        "                                            from the main analysis pipeline.\n",
        "        benchmarks (Dict[str, Dict[str, Any]]): A dictionary where each key is a\n",
        "            check name and the value is a dictionary containing the 'path'\n",
        "            (as a tuple), the 'expected' value, and the relative tolerance 'rtol'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A verification report with an overall status and a\n",
        "                        detailed list of results for each individual check.\n",
        "    \"\"\"\n",
        "    # Initialize the report with a default pass status.\n",
        "    logging.info(\"Verifying results against benchmarks...\")\n",
        "    report = {'summary': {'overall_status': 'PASS'}, 'details': []}\n",
        "\n",
        "    # Iterate through each benchmark check defined in the input dictionary.\n",
        "    for check_name, check_params in benchmarks.items():\n",
        "        # --- Step 1: Robustly Extract Actual Value ---\n",
        "        # Use the safe helper function to retrieve the nested value.\n",
        "        actual_value, success = _get_nested_value(analysis_results, check_params['path'])\n",
        "\n",
        "        # If the path was not found, record the failure and continue.\n",
        "        if not success:\n",
        "            report['summary']['overall_status'] = 'FAIL'\n",
        "            report['details'].append({\n",
        "                'check': check_name,\n",
        "                'status': 'PATH_NOT_FOUND',\n",
        "                'path': check_params['path'],\n",
        "                'message': 'The specified path does not exist in the results dictionary.'\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # --- Step 2: Perform Comparison ---\n",
        "        # Extract the expected value and tolerance for the current check.\n",
        "        expected_value = check_params['expected']\n",
        "        relative_tolerance = check_params['rtol']\n",
        "\n",
        "        # Use numpy.isclose for robust floating-point comparison.\n",
        "        is_pass = np.isclose(actual_value, expected_value, rtol=relative_tolerance)\n",
        "\n",
        "        # Determine the status string for the report.\n",
        "        status = 'PASS' if is_pass else 'FAIL'\n",
        "\n",
        "        # If any check fails, update the overall status of the report.\n",
        "        if not is_pass:\n",
        "            report['summary']['overall_status'] = 'FAIL'\n",
        "\n",
        "        # --- Step 3: Append Detailed Results to Report ---\n",
        "        # Store a complete record of the check in the report details.\n",
        "        report['details'].append({\n",
        "            'check': check_name,\n",
        "            'status': status,\n",
        "            'path': check_params['path'],\n",
        "            'actual': actual_value,\n",
        "            'expected': expected_value,\n",
        "            'tolerance': relative_tolerance\n",
        "        })\n",
        "\n",
        "    # Log the final outcome of the verification process.\n",
        "    logging.info(f\"Verification complete. Overall status: {report['summary']['overall_status']}\")\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "class NumpyJSONEncoder(json.JSONEncoder):\n",
        "    \"\"\"\n",
        "    Custom JSON encoder for NumPy data types.\n",
        "\n",
        "    This encoder handles the serialization of common NumPy types that are not\n",
        "    natively supported by the standard `json` library, such as ndarrays,\n",
        "    integers, and floats.\n",
        "    \"\"\"\n",
        "    def default(self, obj):\n",
        "        # If the object is a NumPy integer, convert it to a Python int.\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        # If the object is a NumPy float, convert it to a Python float.\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        # If the object is a NumPy boolean, convert it to a Python bool.\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        # If the object is a NumPy array, convert it to a nested Python list.\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        # For any other types, let the base class handle it.\n",
        "        return super(NumpyJSONEncoder, self).default(obj)\n",
        "\n",
        "def market_complexity_dashboard_generator(\n",
        "    df_prices: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    run_sensitivity: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Master orchestrator for the entire Market Complexity Dashboard framework.\n",
        "    [Re-implemented to be a complete, fully functional, end-to-end pipeline.]\n",
        "\n",
        "    This function serves as the ultimate single entry point, executing the full\n",
        "    scientific workflow from raw data to final, verified reports. It runs the\n",
        "    main analysis pipeline, optionally conducts a parameter sensitivity analysis,\n",
        "    generates all tables and figures, verifies the results against known\n",
        "    benchmarks, and saves the complete output for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        df_prices (pd.DataFrame): The raw DataFrame of daily closing prices.\n",
        "        study_config (Dict[str, Any]): The configuration dictionary for the study.\n",
        "        run_sensitivity (bool, optional): Whether to run the computationally\n",
        "            intensive parameter sensitivity analysis. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A master dictionary containing all artifacts of the\n",
        "                        study: main results, validation reports, tables,\n",
        "                        figure paths, and sensitivity analysis results.\n",
        "    \"\"\"\n",
        "    # --- a. Run the main analysis pipeline and validate its raw output ---\n",
        "    # Execute the primary, end-to-end analysis.\n",
        "    main_results = run_complexity_analysis_pipeline(df_prices, study_config)\n",
        "    # Perform a quality control check on the generated results.\n",
        "    pipeline_validation_report = validate_analysis_results(main_results)\n",
        "\n",
        "    # --- b. Optionally, run the robustness analysis ---\n",
        "    sensitivity_results_df = None\n",
        "    if run_sensitivity:\n",
        "        # Define a sample parameter grid for the sensitivity analysis.\n",
        "        param_grid = {\n",
        "            'analytical_modules.rcmse_analysis.hyperparameters.embedding_dimension_m': [2, 3],\n",
        "            'analytical_modules.mdfa_analysis.hyperparameters.polynomial_order': [1, 2]\n",
        "        }\n",
        "        sensitivity_results_df = run_parameter_sensitivity_analysis(df_prices, study_config, param_grid)\n",
        "\n",
        "    # --- c. Execute Task 8: Output Generation and Verification ---\n",
        "    # Step 8.1: Generate summary tables from the main results.\n",
        "    summary_tables = generate_summary_tables(main_results)\n",
        "\n",
        "    # Step 8.2: Generate and save publication-quality visualizations.\n",
        "    figure_paths = generate_publication_visualizations(main_results)\n",
        "\n",
        "    # Step 8.3: Verify key results against established benchmarks.\n",
        "    # Define benchmarks with the robust tuple-based path structure.\n",
        "    # Asset names must exactly match the DataFrame column names.\n",
        "    asset_names = df_prices.columns\n",
        "    benchmarks = {\n",
        "        'BTC_RCMSE_Complexity': {\n",
        "            'path': (asset_names[0], 'rcmse', 'total_complexity'),\n",
        "            'expected': 74.66,\n",
        "            'rtol': 0.15 # 15% tolerance due to potential minor implementation differences\n",
        "        },\n",
        "        'BTC_MFDFA_Width': {\n",
        "            'path': (asset_names[0], 'mdfa', 'singularity_spectrum', 'delta_alpha'),\n",
        "            'expected': 0.62,\n",
        "            'rtol': 0.15 # 15% tolerance\n",
        "        }\n",
        "    }\n",
        "    benchmark_verification_report = verify_results_against_benchmarks(main_results, benchmarks)\n",
        "\n",
        "    # --- d. Assemble the final dashboard dictionary and save results ---\n",
        "    # Create the final, comprehensive output object.\n",
        "    final_dashboard = {\n",
        "        'main_analysis_results': main_results,\n",
        "        'pipeline_validation_report': pipeline_validation_report,\n",
        "        'sensitivity_analysis_results': sensitivity_results_df,\n",
        "        'summary_tables': summary_tables,\n",
        "        'figure_paths': figure_paths,\n",
        "        'benchmark_verification_report': benchmark_verification_report\n",
        "    }\n",
        "\n",
        "    # --- Data Persistence for Reproducibility ---\n",
        "    # Define the output directory for raw data.\n",
        "    output_dir = 'results/data'\n",
        "    # Create the directory if it does not exist.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # Define the full path for the output file.\n",
        "    results_filepath = os.path.join(output_dir, 'main_analysis_results.json')\n",
        "\n",
        "    # Save the main results dictionary to a JSON file.\n",
        "    # Use the custom NumpyJSONEncoder to handle NumPy data types.\n",
        "    logging.info(f\"Saving complete results to '{results_filepath}'...\")\n",
        "    with open(results_filepath, 'w') as f:\n",
        "        json.dump(main_results, f, indent=4, cls=NumpyJSONEncoder)\n",
        "    logging.info(\"Results saved successfully.\")\n",
        "\n",
        "    return final_dashboard\n"
      ],
      "metadata": {
        "id": "Yz6Bjx65wzr5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}